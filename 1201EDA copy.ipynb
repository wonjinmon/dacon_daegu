{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    #pythonhashseed 환경변수 설정\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "light_df = pd.read_csv('./data/external_open/대구 보안등 정보.csv', encoding='cp949')[['설치개수', '소재지지번주소']]\n",
    "\n",
    "location_pattern = r'(\\S+) (\\S+) (\\S+) (\\S+)'\n",
    "#공백으로 구분된 네 개의 비공백 문자열을 찾음\n",
    "\n",
    "light_df[['도시', '구', '동', '번지']] = light_df['소재지지번주소'].str.extract(location_pattern)\n",
    "light_df = light_df.drop(columns=['소재지지번주소', '번지'])\n",
    "\n",
    "light_df = light_df.groupby(['도시', '구', '동']).sum().reset_index()\n",
    "#도시, 구, 동 열을 기준으로 데이터를 그룹화 함.\n",
    "#sum()을 해주는 이유는 각 데이터의 도시, 구, 동에 대한 보안등의 개수 합\n",
    "light_df.reset_index(inplace=True, drop=True)\n",
    "#reset_index(): 데이터프레임의 인덱스 초기화.\n",
    "#inplace = true는 변경사항을 light_df에 바로 적용(새로운 df가 아닌 기존 df) / drop = true 인덱스 새 열 추가 안함\n",
    "\n",
    "child_area_df = pd.read_csv('./data/external_open/대구 어린이 보호 구역 정보.csv', encoding='cp949')[['소재지지번주소']]\n",
    "#중복된 값이 있으면 값이 겹칠 수 있으니 미리 중복 제거 \n",
    "child_area_df['School Zone'] = 1\n",
    "\n",
    "location_pattern = r'(\\S+) (\\S+) (\\S+) (\\S+)'\n",
    "\n",
    "child_area_df[['도시', '구', '동', '번지']] = child_area_df['소재지지번주소'].str.extract(location_pattern)\n",
    "child_area_df = child_area_df.drop(columns=['소재지지번주소', '번지'])\n",
    "\n",
    "child_area_df = child_area_df.groupby(['도시', '구', '동']).sum().reset_index()\n",
    "child_area_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "parking_df = pd.read_csv('./data/external_open/대구 주차장 정보.csv', encoding='cp949')[['소재지지번주소', '급지구분']]\n",
    "parking_df = pd.get_dummies(parking_df, columns=['급지구분'])\n",
    "#parking_df값들을 one_hot encoding으로 진행\n",
    "\n",
    "location_pattern = r'(\\S+) (\\S+) (\\S+) (\\S+)'\n",
    "\n",
    "parking_df[['도시', '구', '동', '번지']] = parking_df['소재지지번주소'].str.extract(location_pattern)\n",
    "parking_df = parking_df.drop(columns=['소재지지번주소', '번지'])\n",
    "\n",
    "parking_df = parking_df.groupby(['도시', '구', '동']).sum().reset_index()\n",
    "parking_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>사고일시</th>\n",
       "      <th>요일</th>\n",
       "      <th>기상상태</th>\n",
       "      <th>시군구</th>\n",
       "      <th>도로형태</th>\n",
       "      <th>노면상태</th>\n",
       "      <th>사고유형</th>\n",
       "      <th>사고유형 - 세부분류</th>\n",
       "      <th>법규위반</th>\n",
       "      <th>가해운전자 차종</th>\n",
       "      <th>가해운전자 성별</th>\n",
       "      <th>가해운전자 연령</th>\n",
       "      <th>가해운전자 상해정도</th>\n",
       "      <th>피해운전자 차종</th>\n",
       "      <th>피해운전자 성별</th>\n",
       "      <th>피해운전자 연령</th>\n",
       "      <th>피해운전자 상해정도</th>\n",
       "      <th>사망자수</th>\n",
       "      <th>중상자수</th>\n",
       "      <th>경상자수</th>\n",
       "      <th>부상자수</th>\n",
       "      <th>ECLO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACCIDENT_00000</td>\n",
       "      <td>2019-01-01 00</td>\n",
       "      <td>화요일</td>\n",
       "      <td>맑음</td>\n",
       "      <td>대구광역시 중구 대신동</td>\n",
       "      <td>단일로 - 기타</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대사람</td>\n",
       "      <td>길가장자리구역통행중</td>\n",
       "      <td>안전운전불이행</td>\n",
       "      <td>승용</td>\n",
       "      <td>여</td>\n",
       "      <td>51세</td>\n",
       "      <td>상해없음</td>\n",
       "      <td>보행자</td>\n",
       "      <td>여</td>\n",
       "      <td>70세</td>\n",
       "      <td>중상</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACCIDENT_00001</td>\n",
       "      <td>2019-01-01 00</td>\n",
       "      <td>화요일</td>\n",
       "      <td>흐림</td>\n",
       "      <td>대구광역시 달서구 감삼동</td>\n",
       "      <td>단일로 - 기타</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대사람</td>\n",
       "      <td>보도통행중</td>\n",
       "      <td>기타</td>\n",
       "      <td>승용</td>\n",
       "      <td>남</td>\n",
       "      <td>39세</td>\n",
       "      <td>상해없음</td>\n",
       "      <td>보행자</td>\n",
       "      <td>남</td>\n",
       "      <td>61세</td>\n",
       "      <td>경상</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACCIDENT_00002</td>\n",
       "      <td>2019-01-01 01</td>\n",
       "      <td>화요일</td>\n",
       "      <td>맑음</td>\n",
       "      <td>대구광역시 수성구 두산동</td>\n",
       "      <td>단일로 - 기타</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대사람</td>\n",
       "      <td>차도통행중</td>\n",
       "      <td>안전운전불이행</td>\n",
       "      <td>승용</td>\n",
       "      <td>남</td>\n",
       "      <td>70세</td>\n",
       "      <td>상해없음</td>\n",
       "      <td>보행자</td>\n",
       "      <td>남</td>\n",
       "      <td>38세</td>\n",
       "      <td>경상</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACCIDENT_00003</td>\n",
       "      <td>2019-01-01 02</td>\n",
       "      <td>화요일</td>\n",
       "      <td>맑음</td>\n",
       "      <td>대구광역시 북구 복현동</td>\n",
       "      <td>단일로 - 기타</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대차</td>\n",
       "      <td>추돌</td>\n",
       "      <td>안전운전불이행</td>\n",
       "      <td>승용</td>\n",
       "      <td>남</td>\n",
       "      <td>49세</td>\n",
       "      <td>상해없음</td>\n",
       "      <td>승용</td>\n",
       "      <td>남</td>\n",
       "      <td>36세</td>\n",
       "      <td>중상</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACCIDENT_00004</td>\n",
       "      <td>2019-01-01 04</td>\n",
       "      <td>화요일</td>\n",
       "      <td>맑음</td>\n",
       "      <td>대구광역시 동구 신암동</td>\n",
       "      <td>단일로 - 기타</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대차</td>\n",
       "      <td>추돌</td>\n",
       "      <td>안전운전불이행</td>\n",
       "      <td>승용</td>\n",
       "      <td>남</td>\n",
       "      <td>30세</td>\n",
       "      <td>상해없음</td>\n",
       "      <td>승용</td>\n",
       "      <td>남</td>\n",
       "      <td>52세</td>\n",
       "      <td>경상</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID           사고일시   요일 기상상태            시군구      도로형태 노면상태  \\\n",
       "0  ACCIDENT_00000  2019-01-01 00  화요일   맑음   대구광역시 중구 대신동  단일로 - 기타   건조   \n",
       "1  ACCIDENT_00001  2019-01-01 00  화요일   흐림  대구광역시 달서구 감삼동  단일로 - 기타   건조   \n",
       "2  ACCIDENT_00002  2019-01-01 01  화요일   맑음  대구광역시 수성구 두산동  단일로 - 기타   건조   \n",
       "3  ACCIDENT_00003  2019-01-01 02  화요일   맑음   대구광역시 북구 복현동  단일로 - 기타   건조   \n",
       "4  ACCIDENT_00004  2019-01-01 04  화요일   맑음   대구광역시 동구 신암동  단일로 - 기타   건조   \n",
       "\n",
       "   사고유형 사고유형 - 세부분류     법규위반 가해운전자 차종 가해운전자 성별 가해운전자 연령 가해운전자 상해정도 피해운전자 차종  \\\n",
       "0  차대사람  길가장자리구역통행중  안전운전불이행       승용        여      51세       상해없음      보행자   \n",
       "1  차대사람       보도통행중       기타       승용        남      39세       상해없음      보행자   \n",
       "2  차대사람       차도통행중  안전운전불이행       승용        남      70세       상해없음      보행자   \n",
       "3   차대차          추돌  안전운전불이행       승용        남      49세       상해없음       승용   \n",
       "4   차대차          추돌  안전운전불이행       승용        남      30세       상해없음       승용   \n",
       "\n",
       "  피해운전자 성별 피해운전자 연령 피해운전자 상해정도  사망자수  중상자수  경상자수  부상자수  ECLO  \n",
       "0        여      70세         중상     0     1     0     0     5  \n",
       "1        남      61세         경상     0     0     1     0     3  \n",
       "2        남      38세         경상     0     0     1     0     3  \n",
       "3        남      36세         중상     0     1     0     0     5  \n",
       "4        남      52세         경상     0     0     1     0     3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>사고일시</th>\n",
       "      <th>요일</th>\n",
       "      <th>기상상태</th>\n",
       "      <th>시군구</th>\n",
       "      <th>도로형태</th>\n",
       "      <th>노면상태</th>\n",
       "      <th>사고유형</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACCIDENT_39609</td>\n",
       "      <td>2022-01-01 01</td>\n",
       "      <td>토요일</td>\n",
       "      <td>맑음</td>\n",
       "      <td>대구광역시 수성구 상동</td>\n",
       "      <td>교차로 - 교차로안</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대사람</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACCIDENT_39610</td>\n",
       "      <td>2022-01-01 01</td>\n",
       "      <td>토요일</td>\n",
       "      <td>맑음</td>\n",
       "      <td>대구광역시 수성구 지산동</td>\n",
       "      <td>단일로 - 기타</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대사람</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACCIDENT_39611</td>\n",
       "      <td>2022-01-01 04</td>\n",
       "      <td>토요일</td>\n",
       "      <td>맑음</td>\n",
       "      <td>대구광역시 수성구 수성동2가</td>\n",
       "      <td>교차로 - 교차로안</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대차</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACCIDENT_39612</td>\n",
       "      <td>2022-01-01 04</td>\n",
       "      <td>토요일</td>\n",
       "      <td>맑음</td>\n",
       "      <td>대구광역시 수성구 신매동</td>\n",
       "      <td>단일로 - 기타</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대차</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACCIDENT_39613</td>\n",
       "      <td>2022-01-01 06</td>\n",
       "      <td>토요일</td>\n",
       "      <td>맑음</td>\n",
       "      <td>대구광역시 달서구 감삼동</td>\n",
       "      <td>교차로 - 교차로안</td>\n",
       "      <td>건조</td>\n",
       "      <td>차대차</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID           사고일시   요일 기상상태              시군구        도로형태 노면상태  \\\n",
       "0  ACCIDENT_39609  2022-01-01 01  토요일   맑음     대구광역시 수성구 상동  교차로 - 교차로안   건조   \n",
       "1  ACCIDENT_39610  2022-01-01 01  토요일   맑음    대구광역시 수성구 지산동    단일로 - 기타   건조   \n",
       "2  ACCIDENT_39611  2022-01-01 04  토요일   맑음  대구광역시 수성구 수성동2가  교차로 - 교차로안   건조   \n",
       "3  ACCIDENT_39612  2022-01-01 04  토요일   맑음    대구광역시 수성구 신매동    단일로 - 기타   건조   \n",
       "4  ACCIDENT_39613  2022-01-01 06  토요일   맑음    대구광역시 달서구 감삼동  교차로 - 교차로안   건조   \n",
       "\n",
       "   사고유형  \n",
       "0  차대사람  \n",
       "1  차대사람  \n",
       "2   차대차  \n",
       "3   차대차  \n",
       "4   차대차  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_org = pd.read_csv(\"./data/train.csv\")\n",
    "test_org = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "countrywide_org = pd.read_csv('./data/external_open/countrywide_accident.csv')\n",
    "countrywide_org = countrywide_org[:300000]\n",
    "display(train_org.head())\n",
    "display(test_org.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_org.copy()\n",
    "test_df = test_org.copy()\n",
    "countrywide_df = countrywide_org.copy()\n",
    "\n",
    "time_pattern = r'(\\d{4})-(\\d{1,2})-(\\d{1,2}) (\\d{1,2})' \n",
    "#날짜와 시간을 추출하기 위한 정규표현식\n",
    "# \\d{4}는 연도(4자리 숫자), \\d{1,2}는 월과 일(1자리 또는 2자리 숫자), 그리고 \\d{1,2}는 시간(1자리 또는 2자리 숫자)\n",
    "\n",
    "train_df[['연', '월', '일', '시간']] = train_org['사고일시'].str.extract(time_pattern)\n",
    "                                                                            #pd.to_numeric함수는 숫자처럼 보이는 문자들을 숫자로 바꿔줌\n",
    "train_df[['연', '월', '일', '시간']] = train_df[['연', '월', '일', '시간']].apply(pd.to_numeric) # 추출된 문자열을 수치화\n",
    "train_df = train_df.drop(columns=['사고일시']) # 정보 추출이 완료된 '사고일시' 컬럼은 제거\n",
    "\n",
    "# 해당 과정을 test_x에 대해서도 반복해줍니다 \n",
    "test_df[['연', '월', '일', '시간']] = test_org['사고일시'].str.extract(time_pattern)\n",
    "test_df[['연', '월', '일', '시간']] = test_df[['연', '월', '일', '시간']].apply(pd.to_numeric)\n",
    "test_df = test_df.drop(columns=['사고일시'])\n",
    "\n",
    "countrywide_df[['연', '월', '일', '시간']] = countrywide_org['사고일시'].str.extract(time_pattern)\n",
    "countrywide_df[['연', '월', '일', '시간']] = countrywide_df[['연', '월', '일', '시간']].apply(pd.to_numeric)\n",
    "countrywide_df = countrywide_df.drop(columns=['사고일시'])\n",
    "\n",
    "location_pattern = r'(\\S+) (\\S+) (\\S+)'\n",
    "\n",
    "train_df[['도시', '구', '동']] = train_org['시군구'].str.extract(location_pattern)\n",
    "train_df = train_df.drop(columns=['시군구'])\n",
    "\n",
    "test_df[['도시', '구', '동']] = test_org['시군구'].str.extract(location_pattern)\n",
    "test_df = test_df.drop(columns=['시군구'])\n",
    "\n",
    "countrywide_df[['도시', '구', '동']] = countrywide_org['시군구'].str.extract(location_pattern)\n",
    "countrywide_df = countrywide_df.drop(columns=['시군구'])\n",
    "\n",
    "\n",
    "road_pattern = r'(.+) - (.+)'\n",
    "\n",
    "train_df[['도로형태1', '도로형태2']] = train_org['도로형태'].str.extract(road_pattern)\n",
    "train_df = train_df.drop(columns=['도로형태'])\n",
    "\n",
    "test_df[['도로형태1', '도로형태2']] = test_org['도로형태'].str.extract(road_pattern)\n",
    "test_df = test_df.drop(columns=['도로형태'])\n",
    "\n",
    "countrywide_df[['도로형태1', '도로형태2']] = countrywide_org['도로형태'].str.extract(road_pattern)\n",
    "countrywide_df = countrywide_df.drop(columns=['도로형태'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39609, 34) (10963, 19) (300000, 34)\n"
     ]
    }
   ],
   "source": [
    "# train_df와 test_df에, light_df와 child_area_df, parking_df를 merge하세요.\n",
    "train_df = pd.merge(train_df, light_df, how='left', on=['도시', '구', '동'])\n",
    "train_df = pd.merge(train_df, child_area_df, how='left', on=['도시', '구', '동'])\n",
    "train_df = pd.merge(train_df, parking_df, how='left', on=['도시', '구', '동'])\n",
    "\n",
    "test_df = pd.merge(test_df, light_df, how='left', on=['도시', '구', '동'])\n",
    "test_df = pd.merge(test_df, child_area_df, how='left', on=['도시', '구', '동'])\n",
    "test_df = pd.merge(test_df, parking_df, how='left', on=['도시', '구', '동'])\n",
    "\n",
    "\n",
    "countrywide_df = pd.merge(countrywide_df, light_df, how='left', on=['도시', '구', '동'])\n",
    "countrywide_df = pd.merge(countrywide_df, child_area_df, how='left', on=['도시', '구', '동'])\n",
    "countrywide_df = pd.merge(countrywide_df, parking_df, how='left', on=['도시', '구', '동'])\n",
    "\n",
    "print(train_df.shape, test_df.shape, countrywide_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"./data/train_data_total.csv\", encoding=\"cp949\")\n",
    "test_df.to_csv(\"./data/test_data_total.csv\", encoding=\"cp949\")\n",
    "\n",
    "countrywide_df.to_csv(\"./data/countrywide_data_total.csv\", encoding=\"cp949\")\n",
    "\n",
    "total_df = pd.concat([train_df,countrywide_df])\n",
    "total_df.shape\n",
    "\n",
    "print(train_df.columns)\n",
    "print(test_df.columns)\n",
    "print(countrywide_df.columns)\n",
    "\n",
    "test_x = test_df.drop(columns=['ID']).copy() #column ID열 제거하고 붙여넣기\n",
    "train_x = total_df[test_x.columns].copy() #test.columns 값만 넣기\n",
    "train_y = total_df['ECLO'].copy()\n",
    "\n",
    "# test_x = test_df.drop(columns=['ID']).copy() #column ID열 제거하고 붙여넣기\n",
    "# train_x = train_df[test_x.columns].copy() #test.columns 값만 넣기\n",
    "# train_y = train_df['ECLO'].copy()\n",
    "\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(train_x.columns)\n",
    "# countrywide_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', '요일', '기상상태', '노면상태', '사고유형', '사고유형 - 세부분류', '법규위반', '가해운전자 차종',\n",
      "       '가해운전자 성별', '가해운전자 연령', '가해운전자 상해정도', '피해운전자 차종', '피해운전자 성별',\n",
      "       '피해운전자 연령', '피해운전자 상해정도', '사망자수', '중상자수', '경상자수', '부상자수', 'ECLO', '연',\n",
      "       '월', '일', '시간', '도시', '구', '동', '도로형태1', '도로형태2', '설치개수', 'School Zone',\n",
      "       '급지구분_1', '급지구분_2', '급지구분_3'],\n",
      "      dtype='object')\n",
      "Index(['ID', '요일', '기상상태', '노면상태', '사고유형', '연', '월', '일', '시간', '도시', '구', '동',\n",
      "       '도로형태1', '도로형태2', '설치개수', 'School Zone', '급지구분_1', '급지구분_2', '급지구분_3'],\n",
      "      dtype='object')\n",
      "(39609, 18) (39609,)\n",
      "Index(['요일', '기상상태', '노면상태', '사고유형', '연', '월', '일', '시간', '도시', '구', '동',\n",
      "       '도로형태1', '도로형태2', '설치개수', 'School Zone', '급지구분_1', '급지구분_2', '급지구분_3'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# train_df.to_csv(\"./data/train_data_total.csv\", encoding=\"cp949\")\n",
    "# test_df.to_csv(\"./data/test_data_total.csv\", encoding=\"cp949\")\n",
    "\n",
    "# countrywide_df.to_csv(\"./data/countrywide_data_total.csv\", encoding=\"cp949\")\n",
    "\n",
    "# # total_df = pd.concat([train_df,countrywide_df])\n",
    "# # total_df.shape\n",
    "\n",
    "# print(train_df.columns)\n",
    "# print(test_df.columns)\n",
    "# # print(countrywide_df.columns)\n",
    "\n",
    "# test_x = test_df.drop(columns=['ID']).copy() #column ID열 제거하고 붙여넣기\n",
    "# train_x = train_df[test_x.columns].copy() #test.columns 값만 넣기\n",
    "# train_y = train_df['ECLO'].copy()\n",
    "\n",
    "# # test_x = test_df.drop(columns=['ID']).copy() #column ID열 제거하고 붙여넣기\n",
    "# # train_x = train_df[test_x.columns].copy() #test.columns 값만 넣기\n",
    "# # train_y = train_df['ECLO'].copy()\n",
    "\n",
    "# print(train_x.shape, train_y.shape)\n",
    "# print(train_x.columns)\n",
    "# # countrywide_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "\n",
    "\n",
    "# ohe = OneHotEncoder(sparse=False)\n",
    "# train_gu_name = ohe.fit_transform(train_x[[\"구\"]])\n",
    "# train_gu_name = pd.DataFrame(train_gu_name, columns=[col for col in ohe.categories_[0]])\n",
    "# train_x = pd.concat([train_x.drop(columns=['구']), train_gu_name], axis=1)\n",
    "\n",
    "# test_gu_name = ohe.fit_transform(test_x[[\"구\"]])\n",
    "# test_gu_name = pd.DataFrame(test_gu_name, columns=[col for col in ohe.categories_[0]])\n",
    "# test_x = pd.concat([test_x.drop(columns=['구']), test_gu_name], axis=1)\n",
    "\n",
    "# display(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['요일', '기상상태', '노면상태', '사고유형', '도시', '구', '동', '도로형태1', '도로형태2']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>요일</th>\n",
       "      <th>기상상태</th>\n",
       "      <th>노면상태</th>\n",
       "      <th>사고유형</th>\n",
       "      <th>연</th>\n",
       "      <th>월</th>\n",
       "      <th>일</th>\n",
       "      <th>시간</th>\n",
       "      <th>도시</th>\n",
       "      <th>구</th>\n",
       "      <th>동</th>\n",
       "      <th>도로형태1</th>\n",
       "      <th>도로형태2</th>\n",
       "      <th>설치개수</th>\n",
       "      <th>School Zone</th>\n",
       "      <th>급지구분_1</th>\n",
       "      <th>급지구분_2</th>\n",
       "      <th>급지구분_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.627926</td>\n",
       "      <td>4.712888</td>\n",
       "      <td>4.712878</td>\n",
       "      <td>3.817650</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.726704</td>\n",
       "      <td>4.541610</td>\n",
       "      <td>4.282449</td>\n",
       "      <td>4.671841</td>\n",
       "      <td>4.599599</td>\n",
       "      <td>391.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.627926</td>\n",
       "      <td>4.779150</td>\n",
       "      <td>4.712878</td>\n",
       "      <td>3.817650</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.726704</td>\n",
       "      <td>4.618441</td>\n",
       "      <td>4.738938</td>\n",
       "      <td>4.671841</td>\n",
       "      <td>4.599599</td>\n",
       "      <td>932.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.627926</td>\n",
       "      <td>4.712888</td>\n",
       "      <td>4.712878</td>\n",
       "      <td>3.817650</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.726704</td>\n",
       "      <td>4.727300</td>\n",
       "      <td>4.842715</td>\n",
       "      <td>4.671841</td>\n",
       "      <td>4.599599</td>\n",
       "      <td>473.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.627926</td>\n",
       "      <td>4.712888</td>\n",
       "      <td>4.712878</td>\n",
       "      <td>4.944597</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4.726704</td>\n",
       "      <td>4.687669</td>\n",
       "      <td>4.208920</td>\n",
       "      <td>4.671841</td>\n",
       "      <td>4.599599</td>\n",
       "      <td>534.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.627926</td>\n",
       "      <td>4.712888</td>\n",
       "      <td>4.712878</td>\n",
       "      <td>4.944597</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4.726704</td>\n",
       "      <td>4.889534</td>\n",
       "      <td>4.549091</td>\n",
       "      <td>4.671841</td>\n",
       "      <td>4.599599</td>\n",
       "      <td>2057.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         요일      기상상태      노면상태      사고유형     연  월  일  시간        도시         구  \\\n",
       "0  4.627926  4.712888  4.712878  3.817650  2019  1  1   0  4.726704  4.541610   \n",
       "1  4.627926  4.779150  4.712878  3.817650  2019  1  1   0  4.726704  4.618441   \n",
       "2  4.627926  4.712888  4.712878  3.817650  2019  1  1   1  4.726704  4.727300   \n",
       "3  4.627926  4.712888  4.712878  4.944597  2019  1  1   2  4.726704  4.687669   \n",
       "4  4.627926  4.712888  4.712878  4.944597  2019  1  1   4  4.726704  4.889534   \n",
       "\n",
       "          동     도로형태1     도로형태2    설치개수  School Zone  급지구분_1  급지구분_2  급지구분_3  \n",
       "0  4.282449  4.671841  4.599599   391.0          2.0    11.0     0.0     0.0  \n",
       "1  4.738938  4.671841  4.599599   932.0          NaN     0.0     1.0     3.0  \n",
       "2  4.842715  4.671841  4.599599   473.0          5.0     NaN     NaN     NaN  \n",
       "3  4.208920  4.671841  4.599599   534.0         11.0     0.0     9.0     5.0  \n",
       "4  4.549091  4.671841  4.599599  2057.0          NaN     0.0     1.0     0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>요일</th>\n",
       "      <th>기상상태</th>\n",
       "      <th>노면상태</th>\n",
       "      <th>사고유형</th>\n",
       "      <th>연</th>\n",
       "      <th>월</th>\n",
       "      <th>일</th>\n",
       "      <th>시간</th>\n",
       "      <th>도시</th>\n",
       "      <th>구</th>\n",
       "      <th>동</th>\n",
       "      <th>도로형태1</th>\n",
       "      <th>도로형태2</th>\n",
       "      <th>설치개수</th>\n",
       "      <th>School Zone</th>\n",
       "      <th>급지구분_1</th>\n",
       "      <th>급지구분_2</th>\n",
       "      <th>급지구분_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.920811</td>\n",
       "      <td>4.712888</td>\n",
       "      <td>4.712878</td>\n",
       "      <td>3.817650</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.726704</td>\n",
       "      <td>4.727300</td>\n",
       "      <td>4.881657</td>\n",
       "      <td>4.882281</td>\n",
       "      <td>5.006142</td>\n",
       "      <td>700.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.920811</td>\n",
       "      <td>4.712888</td>\n",
       "      <td>4.712878</td>\n",
       "      <td>3.817650</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.726704</td>\n",
       "      <td>4.727300</td>\n",
       "      <td>4.563008</td>\n",
       "      <td>4.671841</td>\n",
       "      <td>4.599599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.920811</td>\n",
       "      <td>4.712888</td>\n",
       "      <td>4.712878</td>\n",
       "      <td>4.944597</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4.726704</td>\n",
       "      <td>4.727300</td>\n",
       "      <td>4.945578</td>\n",
       "      <td>4.882281</td>\n",
       "      <td>5.006142</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.920811</td>\n",
       "      <td>4.712888</td>\n",
       "      <td>4.712878</td>\n",
       "      <td>4.944597</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4.726704</td>\n",
       "      <td>4.727300</td>\n",
       "      <td>4.438172</td>\n",
       "      <td>4.671841</td>\n",
       "      <td>4.599599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.920811</td>\n",
       "      <td>4.712888</td>\n",
       "      <td>4.712878</td>\n",
       "      <td>4.944597</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.726704</td>\n",
       "      <td>4.618441</td>\n",
       "      <td>4.738938</td>\n",
       "      <td>4.882281</td>\n",
       "      <td>5.006142</td>\n",
       "      <td>932.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         요일      기상상태      노면상태      사고유형     연  월  일  시간        도시         구  \\\n",
       "0  4.920811  4.712888  4.712878  3.817650  2022  1  1   1  4.726704  4.727300   \n",
       "1  4.920811  4.712888  4.712878  3.817650  2022  1  1   1  4.726704  4.727300   \n",
       "2  4.920811  4.712888  4.712878  4.944597  2022  1  1   4  4.726704  4.727300   \n",
       "3  4.920811  4.712888  4.712878  4.944597  2022  1  1   4  4.726704  4.727300   \n",
       "4  4.920811  4.712888  4.712878  4.944597  2022  1  1   6  4.726704  4.618441   \n",
       "\n",
       "          동     도로형태1     도로형태2   설치개수  School Zone  급지구분_1  급지구분_2  급지구분_3  \n",
       "0  4.881657  4.882281  5.006142  700.0          5.0     NaN     NaN     NaN  \n",
       "1  4.563008  4.671841  4.599599    NaN         10.0     0.0     0.0     2.0  \n",
       "2  4.945578  4.882281  5.006142    NaN          1.0     NaN     NaN     NaN  \n",
       "3  4.438172  4.671841  4.599599    NaN          7.0     0.0     2.0     1.0  \n",
       "4  4.738938  4.882281  5.006142  932.0          NaN     0.0     1.0     3.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "\n",
    "categorical_features = list(train_x.dtypes[train_x.dtypes == \"object\"].index) #object값 list에 넣음\n",
    "# 추출된 문자열 변수 확인\n",
    "display(categorical_features) #object값 출력\n",
    "\n",
    "for i in categorical_features: #인코딩 적용한 값 반환\n",
    "    le = TargetEncoder(cols=[i])\n",
    "    train_x[i] = le.fit_transform(train_x[i], train_y)\n",
    "    test_x[i] = le.transform(test_x[i])\n",
    "#target encdoer 주의사항: train은 fit_transform인 반면, test는 transform만 진행!\n",
    "\n",
    "    \n",
    "display(train_x.head())\n",
    "display(test_x.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x.fillna(0, inplace=True)\n",
    "# test_x.fillna(0, inplace=True)\n",
    "\n",
    "train_x['설치개수'] = train_x['설치개수'].fillna(train_x['설치개수'].mean())\n",
    "train_x['School Zone'] = train_x['School Zone'].fillna(train_x['School Zone'].mean())\n",
    "\n",
    "train_x['급지구분_1'] = train_x['급지구분_1'].fillna(train_x['급지구분_1'].mean())\n",
    "train_x['급지구분_2'] = train_x['급지구분_2'].fillna(train_x['급지구분_2'].mean())\n",
    "train_x['급지구분_3'] = train_x['급지구분_3'].fillna(train_x['급지구분_3'].mean())\n",
    "\n",
    "\n",
    "test_x['설치개수'] = test_x['설치개수'].fillna(test_x['설치개수'].mean())\n",
    "test_x['School Zone'] = test_x['School Zone'].fillna(test_x['School Zone'].mean())\n",
    "\n",
    "test_x['급지구분_1'] = test_x['급지구분_1'].fillna(test_x['급지구분_1'].mean())\n",
    "test_x['급지구분_2'] = test_x['급지구분_2'].fillna(test_x['급지구분_2'].mean())\n",
    "test_x['급지구분_3'] = test_x['급지구분_3'].fillna(test_x['급지구분_3'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l1 as l1_regularizer, l2 as l2_regularizer\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    # y_true = tf.cast(y_true, tf.float32)\n",
    "    # y_pred = tf.cast(y_pred, tf.float32)\n",
    "    # squared_error = tf.square(tf.math.log1p(y_pred) - tf.math.log1p(y_true))\n",
    "    # return tf.sqrt(tf.reduce_mean(squared_error))\n",
    "    \n",
    "    y_true = tf.maximum(tf.cast(y_true, tf.float32), 0)\n",
    "    y_pred = tf.maximum(tf.cast(y_pred, tf.float32), 0)\n",
    "    squared_error = tf.square(tf.math.log1p(y_pred) - tf.math.log1p(y_true))\n",
    "    \n",
    "    return tf.sqrt(tf.reduce_mean(squared_error))\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    return rmsle(y_true, y_pred)\n",
    "\n",
    "def metric_fn(y_true, y_pred):\n",
    "    return rmsle(y_true, y_pred)\n",
    "\n",
    "callbacks_list = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, verbose=2, mode='min', restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=4, min_lr=0.00001),\n",
    "    tf.keras.callbacks.TerminateOnNaN()\n",
    "] \n",
    "\n",
    "def create_model(l1_reg, l2_reg, learning_rate):\n",
    "    input_layer = tf.keras.Input(shape=(len(train_x.columns),))\n",
    "    x = tf.keras.layers.BatchNormalization(epsilon=0.00001)(input_layer)\n",
    "    x = tf.keras.layers.Dense(24, kernel_regularizer=l1_regularizer(l1_reg))(x)\n",
    "    x = tf.keras.layers.BatchNormalization(epsilon=0.00001)(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(56, kernel_regularizer=l2_regularizer(l2_reg))(x)\n",
    "    x = tf.keras.layers.BatchNormalization(epsilon=0.00001)(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    \n",
    "    # x = tf.keras.layers.Dense(72, kernel_regularizer=l2_regularizer(l2_reg))(x)\n",
    "    # x = tf.keras.layers.BatchNormalization(epsilon=0.00001)(x)\n",
    "    # x = tf.keras.layers.Activation('relu')(x)\n",
    "    \n",
    "    output_layer = tf.keras.layers.Dense(1)(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "                  loss=loss_fn,\n",
    "                  metrics=[metric_fn])\n",
    "    return model\n",
    "\n",
    "best_params = {'batch_size': 128, 'l1_reg': 0.0001, 'l2_reg': 0.0001, 'learning_rate': 0.005}\n",
    "optimized_model = create_model(best_params['l1_reg'], best_params['l2_reg'], best_params['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------0-------------------------\n",
      "Epoch 1/120\n",
      "248/248 [==============================] - 3s 4ms/step - loss: 0.4840 - metric_fn: 0.4731 - val_loss: 2.8523 - val_metric_fn: 2.8419 - lr: 0.0050\n",
      "Epoch 2/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4572 - metric_fn: 0.4473 - val_loss: 0.7640 - val_metric_fn: 0.7546 - lr: 0.0050\n",
      "Epoch 3/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4546 - metric_fn: 0.4456 - val_loss: 0.4577 - val_metric_fn: 0.4491 - lr: 0.0050\n",
      "Epoch 4/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4532 - metric_fn: 0.4450 - val_loss: 0.4526 - val_metric_fn: 0.4448 - lr: 0.0050\n",
      "Epoch 5/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4514 - metric_fn: 0.4442 - val_loss: 0.4530 - val_metric_fn: 0.4460 - lr: 0.0050\n",
      "Epoch 6/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4509 - metric_fn: 0.4442 - val_loss: 0.4546 - val_metric_fn: 0.4480 - lr: 0.0050\n",
      "Epoch 7/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4497 - metric_fn: 0.4434 - val_loss: 0.4522 - val_metric_fn: 0.4460 - lr: 0.0050\n",
      "Epoch 8/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4495 - metric_fn: 0.4436 - val_loss: 0.4497 - val_metric_fn: 0.4439 - lr: 0.0050\n",
      "Epoch 9/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4491 - metric_fn: 0.4435 - val_loss: 0.4501 - val_metric_fn: 0.4445 - lr: 0.0050\n",
      "Epoch 10/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4485 - metric_fn: 0.4433 - val_loss: 0.4517 - val_metric_fn: 0.4466 - lr: 0.0050\n",
      "Epoch 11/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4483 - metric_fn: 0.4434 - val_loss: 0.4498 - val_metric_fn: 0.4449 - lr: 0.0050\n",
      "Epoch 12/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4477 - metric_fn: 0.4431 - val_loss: 0.4472 - val_metric_fn: 0.4426 - lr: 0.0050\n",
      "Epoch 13/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4477 - metric_fn: 0.4434 - val_loss: 0.4479 - val_metric_fn: 0.4436 - lr: 0.0050\n",
      "Epoch 14/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4473 - metric_fn: 0.4431 - val_loss: 0.4491 - val_metric_fn: 0.4450 - lr: 0.0050\n",
      "Epoch 15/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4470 - metric_fn: 0.4431 - val_loss: 0.4473 - val_metric_fn: 0.4432 - lr: 0.0050\n",
      "Epoch 16/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4471 - metric_fn: 0.4433 - val_loss: 0.4468 - val_metric_fn: 0.4428 - lr: 0.0050\n",
      "Epoch 17/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4463 - metric_fn: 0.4426 - val_loss: 0.4472 - val_metric_fn: 0.4435 - lr: 0.0050\n",
      "Epoch 18/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4464 - metric_fn: 0.4429 - val_loss: 0.4466 - val_metric_fn: 0.4431 - lr: 0.0050\n",
      "Epoch 19/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4461 - metric_fn: 0.4428 - val_loss: 0.4472 - val_metric_fn: 0.4438 - lr: 0.0050\n",
      "Epoch 20/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4464 - metric_fn: 0.4432 - val_loss: 0.4492 - val_metric_fn: 0.4458 - lr: 0.0050\n",
      "Epoch 21/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4461 - metric_fn: 0.4428 - val_loss: 0.4464 - val_metric_fn: 0.4432 - lr: 0.0050\n",
      "Epoch 22/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4456 - metric_fn: 0.4428 - val_loss: 0.4473 - val_metric_fn: 0.4442 - lr: 0.0050\n",
      "Epoch 23/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4460 - metric_fn: 0.4432 - val_loss: 0.4463 - val_metric_fn: 0.4434 - lr: 0.0050\n",
      "Epoch 24/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4453 - metric_fn: 0.4424 - val_loss: 0.4470 - val_metric_fn: 0.4442 - lr: 0.0050\n",
      "Epoch 25/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4456 - metric_fn: 0.4429 - val_loss: 0.4467 - val_metric_fn: 0.4439 - lr: 0.0050\n",
      "Epoch 26/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4454 - metric_fn: 0.4428 - val_loss: 0.4474 - val_metric_fn: 0.4446 - lr: 0.0050\n",
      "Epoch 27/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4453 - metric_fn: 0.4428 - val_loss: 0.4463 - val_metric_fn: 0.4438 - lr: 0.0050\n",
      "Epoch 28/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4445 - metric_fn: 0.4422 - val_loss: 0.4457 - val_metric_fn: 0.4433 - lr: 0.0035\n",
      "Epoch 29/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4444 - metric_fn: 0.4421 - val_loss: 0.4454 - val_metric_fn: 0.4431 - lr: 0.0035\n",
      "Epoch 30/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4442 - metric_fn: 0.4419 - val_loss: 0.4447 - val_metric_fn: 0.4426 - lr: 0.0035\n",
      "Epoch 31/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4445 - metric_fn: 0.4426 - val_loss: 0.4451 - val_metric_fn: 0.4430 - lr: 0.0035\n",
      "Epoch 32/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4442 - metric_fn: 0.4423 - val_loss: 0.4454 - val_metric_fn: 0.4433 - lr: 0.0035\n",
      "Epoch 33/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4439 - metric_fn: 0.4420 - val_loss: 0.4475 - val_metric_fn: 0.4455 - lr: 0.0035\n",
      "Epoch 34/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4441 - metric_fn: 0.4421 - val_loss: 0.4443 - val_metric_fn: 0.4423 - lr: 0.0035\n",
      "Epoch 35/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4440 - metric_fn: 0.4422 - val_loss: 0.4448 - val_metric_fn: 0.4429 - lr: 0.0035\n",
      "Epoch 36/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4440 - metric_fn: 0.4422 - val_loss: 0.4450 - val_metric_fn: 0.4430 - lr: 0.0035\n",
      "Epoch 37/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4440 - metric_fn: 0.4422 - val_loss: 0.4457 - val_metric_fn: 0.4438 - lr: 0.0035\n",
      "Epoch 38/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4440 - metric_fn: 0.4422 - val_loss: 0.4449 - val_metric_fn: 0.4430 - lr: 0.0035\n",
      "Epoch 39/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4436 - metric_fn: 0.4419 - val_loss: 0.4453 - val_metric_fn: 0.4435 - lr: 0.0025\n",
      "Epoch 40/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4434 - metric_fn: 0.4418 - val_loss: 0.4449 - val_metric_fn: 0.4432 - lr: 0.0025\n",
      "Epoch 41/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4435 - metric_fn: 0.4419 - val_loss: 0.4457 - val_metric_fn: 0.4439 - lr: 0.0025\n",
      "Epoch 42/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4432 - metric_fn: 0.4417 - val_loss: 0.4444 - val_metric_fn: 0.4427 - lr: 0.0025\n",
      "Epoch 43/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4430 - metric_fn: 0.4413 - val_loss: 0.4446 - val_metric_fn: 0.4430 - lr: 0.0017\n",
      "Epoch 44/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4429 - metric_fn: 0.4414 - val_loss: 0.4445 - val_metric_fn: 0.4429 - lr: 0.0017\n",
      "Epoch 45/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4429 - metric_fn: 0.4414 - val_loss: 0.4447 - val_metric_fn: 0.4432 - lr: 0.0017\n",
      "Epoch 46/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4428 - metric_fn: 0.4414 - val_loss: 0.4446 - val_metric_fn: 0.4431 - lr: 0.0017\n",
      "Epoch 47/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4423 - metric_fn: 0.4409 - val_loss: 0.4445 - val_metric_fn: 0.4429 - lr: 0.0012\n",
      "Epoch 48/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4421 - metric_fn: 0.4408 - val_loss: 0.4447 - val_metric_fn: 0.4432 - lr: 0.0012\n",
      "Epoch 49/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4423 - metric_fn: 0.4410 - val_loss: 0.4447 - val_metric_fn: 0.4432 - lr: 0.0012\n",
      "Epoch 50/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4424 - metric_fn: 0.4411 - val_loss: 0.4440 - val_metric_fn: 0.4425 - lr: 0.0012\n",
      "Epoch 51/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4425 - metric_fn: 0.4411 - val_loss: 0.4446 - val_metric_fn: 0.4431 - lr: 0.0012\n",
      "Epoch 52/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4422 - metric_fn: 0.4410 - val_loss: 0.4446 - val_metric_fn: 0.4432 - lr: 0.0012\n",
      "Epoch 53/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4423 - metric_fn: 0.4411 - val_loss: 0.4442 - val_metric_fn: 0.4428 - lr: 0.0012\n",
      "Epoch 54/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4423 - metric_fn: 0.4409 - val_loss: 0.4447 - val_metric_fn: 0.4433 - lr: 0.0012\n",
      "Epoch 55/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4416 - metric_fn: 0.4403 - val_loss: 0.4446 - val_metric_fn: 0.4432 - lr: 8.4035e-04\n",
      "Epoch 56/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4416 - metric_fn: 0.4404 - val_loss: 0.4445 - val_metric_fn: 0.4431 - lr: 8.4035e-04\n",
      "Epoch 57/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4418 - metric_fn: 0.4404 - val_loss: 0.4442 - val_metric_fn: 0.4429 - lr: 8.4035e-04\n",
      "Epoch 58/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4419 - metric_fn: 0.4407 - val_loss: 0.4445 - val_metric_fn: 0.4432 - lr: 8.4035e-04\n",
      "Epoch 59/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4412 - metric_fn: 0.4400 - val_loss: 0.4449 - val_metric_fn: 0.4435 - lr: 5.8825e-04\n",
      "Epoch 60/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4413 - metric_fn: 0.4401 - val_loss: 0.4442 - val_metric_fn: 0.4429 - lr: 5.8825e-04\n",
      "Epoch 61/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4416 - metric_fn: 0.4404 - val_loss: 0.4441 - val_metric_fn: 0.4428 - lr: 5.8825e-04\n",
      "Epoch 62/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4415 - metric_fn: 0.4403 - val_loss: 0.4443 - val_metric_fn: 0.4430 - lr: 5.8825e-04\n",
      "Epoch 63/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4412 - metric_fn: 0.4401 - val_loss: 0.4444 - val_metric_fn: 0.4431 - lr: 4.1177e-04\n",
      "Epoch 64/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4413 - metric_fn: 0.4402 - val_loss: 0.4444 - val_metric_fn: 0.4431 - lr: 4.1177e-04\n",
      "Epoch 65/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4413 - metric_fn: 0.4403 - val_loss: 0.4444 - val_metric_fn: 0.4431 - lr: 4.1177e-04\n",
      "Epoch 66/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4411 - metric_fn: 0.4399 - val_loss: 0.4441 - val_metric_fn: 0.4428 - lr: 4.1177e-04\n",
      "Epoch 67/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4410 - metric_fn: 0.4397 - val_loss: 0.4443 - val_metric_fn: 0.4431 - lr: 2.8824e-04\n",
      "Epoch 68/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4408 - metric_fn: 0.4397 - val_loss: 0.4443 - val_metric_fn: 0.4430 - lr: 2.8824e-04\n",
      "Epoch 69/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4411 - metric_fn: 0.4400 - val_loss: 0.4441 - val_metric_fn: 0.4429 - lr: 2.8824e-04\n",
      "Epoch 70/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4411 - metric_fn: 0.4399 - val_loss: 0.4444 - val_metric_fn: 0.4431 - lr: 2.8824e-04\n",
      "Epoch 71/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4408 - metric_fn: 0.4398 - val_loss: 0.4443 - val_metric_fn: 0.4430 - lr: 2.0177e-04\n",
      "Epoch 72/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4410 - metric_fn: 0.4398 - val_loss: 0.4443 - val_metric_fn: 0.4430 - lr: 2.0177e-04\n",
      "Epoch 73/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4408 - metric_fn: 0.4395 - val_loss: 0.4442 - val_metric_fn: 0.4430 - lr: 2.0177e-04\n",
      "Epoch 74/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4408 - metric_fn: 0.4396 - val_loss: 0.4442 - val_metric_fn: 0.4430 - lr: 2.0177e-04\n",
      "Epoch 75/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4407 - metric_fn: 0.4395 - val_loss: 0.4442 - val_metric_fn: 0.4430 - lr: 1.4124e-04\n",
      "Epoch 76/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4407 - metric_fn: 0.4396 - val_loss: 0.4443 - val_metric_fn: 0.4430 - lr: 1.4124e-04\n",
      "Epoch 77/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4409 - metric_fn: 0.4398 - val_loss: 0.4443 - val_metric_fn: 0.4431 - lr: 1.4124e-04\n",
      "Epoch 78/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4406 - metric_fn: 0.4395 - val_loss: 0.4443 - val_metric_fn: 0.4431 - lr: 1.4124e-04\n",
      "Epoch 79/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4405 - metric_fn: 0.4395 - val_loss: 0.4443 - val_metric_fn: 0.4431 - lr: 9.8866e-05\n",
      "Epoch 80/120\n",
      "243/248 [============================>.] - ETA: 0s - loss: 0.4408 - metric_fn: 0.4397Restoring model weights from the end of the best epoch: 50.\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4406 - metric_fn: 0.4395 - val_loss: 0.4443 - val_metric_fn: 0.4431 - lr: 9.8866e-05\n",
      "Epoch 80: early stopping\n",
      "343/343 [==============================] - 1s 1ms/step\n",
      "-------------------1-------------------------\n",
      "Epoch 1/120\n",
      "248/248 [==============================] - 2s 4ms/step - loss: 0.5321 - metric_fn: 0.5206 - val_loss: 2.7480 - val_metric_fn: 2.7373 - lr: 0.0050\n",
      "Epoch 2/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4564 - metric_fn: 0.4461 - val_loss: 0.8148 - val_metric_fn: 0.8052 - lr: 0.0050\n",
      "Epoch 3/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4530 - metric_fn: 0.4439 - val_loss: 0.4641 - val_metric_fn: 0.4554 - lr: 0.0050\n",
      "Epoch 4/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4521 - metric_fn: 0.4440 - val_loss: 0.4604 - val_metric_fn: 0.4525 - lr: 0.0050\n",
      "Epoch 5/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4508 - metric_fn: 0.4433 - val_loss: 0.4588 - val_metric_fn: 0.4516 - lr: 0.0050\n",
      "Epoch 6/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4493 - metric_fn: 0.4424 - val_loss: 0.4581 - val_metric_fn: 0.4514 - lr: 0.0050\n",
      "Epoch 7/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4489 - metric_fn: 0.4425 - val_loss: 0.4573 - val_metric_fn: 0.4511 - lr: 0.0050\n",
      "Epoch 8/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4482 - metric_fn: 0.4423 - val_loss: 0.4571 - val_metric_fn: 0.4512 - lr: 0.0050\n",
      "Epoch 9/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4478 - metric_fn: 0.4422 - val_loss: 0.4565 - val_metric_fn: 0.4511 - lr: 0.0050\n",
      "Epoch 10/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4470 - metric_fn: 0.4419 - val_loss: 0.4553 - val_metric_fn: 0.4502 - lr: 0.0050\n",
      "Epoch 11/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4467 - metric_fn: 0.4419 - val_loss: 0.4613 - val_metric_fn: 0.4565 - lr: 0.0050\n",
      "Epoch 12/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4465 - metric_fn: 0.4419 - val_loss: 0.4556 - val_metric_fn: 0.4511 - lr: 0.0050\n",
      "Epoch 13/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4460 - metric_fn: 0.4418 - val_loss: 0.4551 - val_metric_fn: 0.4508 - lr: 0.0050\n",
      "Epoch 14/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4459 - metric_fn: 0.4417 - val_loss: 0.4540 - val_metric_fn: 0.4499 - lr: 0.0050\n",
      "Epoch 15/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4459 - metric_fn: 0.4418 - val_loss: 0.4533 - val_metric_fn: 0.4494 - lr: 0.0050\n",
      "Epoch 16/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4452 - metric_fn: 0.4415 - val_loss: 0.4542 - val_metric_fn: 0.4506 - lr: 0.0050\n",
      "Epoch 17/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4450 - metric_fn: 0.4415 - val_loss: 0.4539 - val_metric_fn: 0.4503 - lr: 0.0050\n",
      "Epoch 18/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4443 - metric_fn: 0.4408 - val_loss: 0.4582 - val_metric_fn: 0.4549 - lr: 0.0050\n",
      "Epoch 19/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4445 - metric_fn: 0.4414 - val_loss: 0.4524 - val_metric_fn: 0.4492 - lr: 0.0050\n",
      "Epoch 20/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4444 - metric_fn: 0.4414 - val_loss: 0.4533 - val_metric_fn: 0.4503 - lr: 0.0050\n",
      "Epoch 21/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4444 - metric_fn: 0.4415 - val_loss: 0.4538 - val_metric_fn: 0.4509 - lr: 0.0050\n",
      "Epoch 22/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4441 - metric_fn: 0.4415 - val_loss: 0.4514 - val_metric_fn: 0.4486 - lr: 0.0050\n",
      "Epoch 23/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4438 - metric_fn: 0.4412 - val_loss: 0.4520 - val_metric_fn: 0.4494 - lr: 0.0050\n",
      "Epoch 24/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4439 - metric_fn: 0.4416 - val_loss: 0.4525 - val_metric_fn: 0.4500 - lr: 0.0050\n",
      "Epoch 25/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4439 - metric_fn: 0.4414 - val_loss: 0.4511 - val_metric_fn: 0.4488 - lr: 0.0050\n",
      "Epoch 26/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4435 - metric_fn: 0.4413 - val_loss: 0.4535 - val_metric_fn: 0.4513 - lr: 0.0050\n",
      "Epoch 27/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4436 - metric_fn: 0.4415 - val_loss: 0.4510 - val_metric_fn: 0.4489 - lr: 0.0050\n",
      "Epoch 28/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4432 - metric_fn: 0.4412 - val_loss: 0.4507 - val_metric_fn: 0.4486 - lr: 0.0050\n",
      "Epoch 29/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4431 - metric_fn: 0.4411 - val_loss: 0.4505 - val_metric_fn: 0.4485 - lr: 0.0050\n",
      "Epoch 30/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4430 - metric_fn: 0.4412 - val_loss: 0.4501 - val_metric_fn: 0.4482 - lr: 0.0050\n",
      "Epoch 31/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4431 - metric_fn: 0.4415 - val_loss: 0.4506 - val_metric_fn: 0.4487 - lr: 0.0050\n",
      "Epoch 32/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4430 - metric_fn: 0.4414 - val_loss: 0.4502 - val_metric_fn: 0.4484 - lr: 0.0050\n",
      "Epoch 33/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4425 - metric_fn: 0.4409 - val_loss: 0.4500 - val_metric_fn: 0.4483 - lr: 0.0050\n",
      "Epoch 34/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4428 - metric_fn: 0.4412 - val_loss: 0.4509 - val_metric_fn: 0.4492 - lr: 0.0050\n",
      "Epoch 35/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4430 - metric_fn: 0.4414 - val_loss: 0.4505 - val_metric_fn: 0.4488 - lr: 0.0050\n",
      "Epoch 36/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4424 - metric_fn: 0.4409 - val_loss: 0.4500 - val_metric_fn: 0.4484 - lr: 0.0050\n",
      "Epoch 37/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4426 - metric_fn: 0.4412 - val_loss: 0.4501 - val_metric_fn: 0.4486 - lr: 0.0050\n",
      "Epoch 38/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4420 - metric_fn: 0.4405 - val_loss: 0.4500 - val_metric_fn: 0.4485 - lr: 0.0035\n",
      "Epoch 39/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4421 - metric_fn: 0.4408 - val_loss: 0.4498 - val_metric_fn: 0.4484 - lr: 0.0035\n",
      "Epoch 40/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4422 - metric_fn: 0.4407 - val_loss: 0.4498 - val_metric_fn: 0.4484 - lr: 0.0035\n",
      "Epoch 41/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4416 - metric_fn: 0.4404 - val_loss: 0.4499 - val_metric_fn: 0.4485 - lr: 0.0035\n",
      "Epoch 42/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4420 - metric_fn: 0.4408 - val_loss: 0.4500 - val_metric_fn: 0.4487 - lr: 0.0035\n",
      "Epoch 43/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4420 - metric_fn: 0.4407 - val_loss: 0.4494 - val_metric_fn: 0.4482 - lr: 0.0035\n",
      "Epoch 44/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4418 - metric_fn: 0.4405 - val_loss: 0.4494 - val_metric_fn: 0.4480 - lr: 0.0035\n",
      "Epoch 45/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4420 - metric_fn: 0.4407 - val_loss: 0.4502 - val_metric_fn: 0.4489 - lr: 0.0035\n",
      "Epoch 46/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4419 - metric_fn: 0.4408 - val_loss: 0.4511 - val_metric_fn: 0.4498 - lr: 0.0035\n",
      "Epoch 47/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4421 - metric_fn: 0.4409 - val_loss: 0.4497 - val_metric_fn: 0.4484 - lr: 0.0035\n",
      "Epoch 48/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4416 - metric_fn: 0.4404 - val_loss: 0.4497 - val_metric_fn: 0.4485 - lr: 0.0025\n",
      "Epoch 49/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4415 - metric_fn: 0.4404 - val_loss: 0.4521 - val_metric_fn: 0.4509 - lr: 0.0025\n",
      "Epoch 50/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4413 - metric_fn: 0.4401 - val_loss: 0.4497 - val_metric_fn: 0.4485 - lr: 0.0025\n",
      "Epoch 51/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4414 - metric_fn: 0.4402 - val_loss: 0.4493 - val_metric_fn: 0.4481 - lr: 0.0025\n",
      "Epoch 52/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4413 - metric_fn: 0.4402 - val_loss: 0.4498 - val_metric_fn: 0.4486 - lr: 0.0025\n",
      "Epoch 53/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4413 - metric_fn: 0.4403 - val_loss: 0.4495 - val_metric_fn: 0.4483 - lr: 0.0025\n",
      "Epoch 54/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4415 - metric_fn: 0.4405 - val_loss: 0.4497 - val_metric_fn: 0.4485 - lr: 0.0025\n",
      "Epoch 55/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4414 - metric_fn: 0.4403 - val_loss: 0.4499 - val_metric_fn: 0.4487 - lr: 0.0025\n",
      "Epoch 56/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4413 - metric_fn: 0.4401 - val_loss: 0.4492 - val_metric_fn: 0.4481 - lr: 0.0017\n",
      "Epoch 57/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4409 - metric_fn: 0.4397 - val_loss: 0.4493 - val_metric_fn: 0.4482 - lr: 0.0017\n",
      "Epoch 58/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4412 - metric_fn: 0.4403 - val_loss: 0.4492 - val_metric_fn: 0.4482 - lr: 0.0017\n",
      "Epoch 59/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4412 - metric_fn: 0.4402 - val_loss: 0.4491 - val_metric_fn: 0.4481 - lr: 0.0017\n",
      "Epoch 60/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4408 - metric_fn: 0.4397 - val_loss: 0.4493 - val_metric_fn: 0.4483 - lr: 0.0017\n",
      "Epoch 61/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4408 - metric_fn: 0.4398 - val_loss: 0.4503 - val_metric_fn: 0.4493 - lr: 0.0017\n",
      "Epoch 62/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4409 - metric_fn: 0.4400 - val_loss: 0.4494 - val_metric_fn: 0.4483 - lr: 0.0017\n",
      "Epoch 63/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4408 - metric_fn: 0.4400 - val_loss: 0.4489 - val_metric_fn: 0.4478 - lr: 0.0017\n",
      "Epoch 64/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4409 - metric_fn: 0.4403 - val_loss: 0.4496 - val_metric_fn: 0.4486 - lr: 0.0017\n",
      "Epoch 65/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4409 - metric_fn: 0.4399 - val_loss: 0.4494 - val_metric_fn: 0.4484 - lr: 0.0017\n",
      "Epoch 66/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4408 - metric_fn: 0.4398 - val_loss: 0.4488 - val_metric_fn: 0.4478 - lr: 0.0017\n",
      "Epoch 67/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4411 - metric_fn: 0.4401 - val_loss: 0.4491 - val_metric_fn: 0.4481 - lr: 0.0017\n",
      "Epoch 68/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4405 - metric_fn: 0.4395 - val_loss: 0.4493 - val_metric_fn: 0.4483 - lr: 0.0012\n",
      "Epoch 69/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4404 - metric_fn: 0.4394 - val_loss: 0.4497 - val_metric_fn: 0.4487 - lr: 0.0012\n",
      "Epoch 70/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4408 - metric_fn: 0.4399 - val_loss: 0.4488 - val_metric_fn: 0.4478 - lr: 0.0012\n",
      "Epoch 71/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4406 - metric_fn: 0.4398 - val_loss: 0.4492 - val_metric_fn: 0.4483 - lr: 0.0012\n",
      "Epoch 72/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4403 - metric_fn: 0.4394 - val_loss: 0.4492 - val_metric_fn: 0.4483 - lr: 8.4035e-04\n",
      "Epoch 73/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4407 - metric_fn: 0.4399 - val_loss: 0.4495 - val_metric_fn: 0.4486 - lr: 8.4035e-04\n",
      "Epoch 74/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4405 - metric_fn: 0.4395 - val_loss: 0.4495 - val_metric_fn: 0.4486 - lr: 8.4035e-04\n",
      "Epoch 75/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4404 - metric_fn: 0.4396 - val_loss: 0.4492 - val_metric_fn: 0.4483 - lr: 8.4035e-04\n",
      "Epoch 76/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4403 - metric_fn: 0.4393 - val_loss: 0.4492 - val_metric_fn: 0.4483 - lr: 5.8825e-04\n",
      "Epoch 77/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4403 - metric_fn: 0.4396 - val_loss: 0.4496 - val_metric_fn: 0.4487 - lr: 5.8825e-04\n",
      "Epoch 78/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4402 - metric_fn: 0.4396 - val_loss: 0.4495 - val_metric_fn: 0.4486 - lr: 5.8825e-04\n",
      "Epoch 79/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4402 - metric_fn: 0.4394 - val_loss: 0.4497 - val_metric_fn: 0.4488 - lr: 5.8825e-04\n",
      "Epoch 80/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4402 - metric_fn: 0.4394 - val_loss: 0.4495 - val_metric_fn: 0.4487 - lr: 4.1177e-04\n",
      "Epoch 81/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4401 - metric_fn: 0.4394 - val_loss: 0.4493 - val_metric_fn: 0.4485 - lr: 4.1177e-04\n",
      "Epoch 82/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4402 - metric_fn: 0.4394 - val_loss: 0.4495 - val_metric_fn: 0.4486 - lr: 4.1177e-04\n",
      "Epoch 83/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4401 - metric_fn: 0.4394 - val_loss: 0.4494 - val_metric_fn: 0.4485 - lr: 4.1177e-04\n",
      "Epoch 84/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4399 - metric_fn: 0.4391 - val_loss: 0.4494 - val_metric_fn: 0.4485 - lr: 2.8824e-04\n",
      "Epoch 85/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4398 - metric_fn: 0.4391 - val_loss: 0.4494 - val_metric_fn: 0.4485 - lr: 2.8824e-04\n",
      "Epoch 86/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4398 - metric_fn: 0.4390 - val_loss: 0.4492 - val_metric_fn: 0.4484 - lr: 2.8824e-04\n",
      "Epoch 87/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4400 - metric_fn: 0.4393 - val_loss: 0.4494 - val_metric_fn: 0.4485 - lr: 2.8824e-04\n",
      "Epoch 88/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4398 - metric_fn: 0.4391 - val_loss: 0.4494 - val_metric_fn: 0.4485 - lr: 2.0177e-04\n",
      "Epoch 89/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4400 - metric_fn: 0.4392 - val_loss: 0.4493 - val_metric_fn: 0.4485 - lr: 2.0177e-04\n",
      "Epoch 90/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4399 - metric_fn: 0.4391 - val_loss: 0.4493 - val_metric_fn: 0.4485 - lr: 2.0177e-04\n",
      "Epoch 91/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4398 - metric_fn: 0.4390 - val_loss: 0.4494 - val_metric_fn: 0.4485 - lr: 2.0177e-04\n",
      "Epoch 92/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4398 - metric_fn: 0.4390 - val_loss: 0.4494 - val_metric_fn: 0.4485 - lr: 1.4124e-04\n",
      "Epoch 93/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4397 - metric_fn: 0.4390 - val_loss: 0.4493 - val_metric_fn: 0.4485 - lr: 1.4124e-04\n",
      "Epoch 94/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4397 - metric_fn: 0.4389 - val_loss: 0.4494 - val_metric_fn: 0.4485 - lr: 1.4124e-04\n",
      "Epoch 95/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4398 - metric_fn: 0.4391 - val_loss: 0.4495 - val_metric_fn: 0.4487 - lr: 1.4124e-04\n",
      "Epoch 96/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4398 - metric_fn: 0.4390 - val_loss: 0.4493 - val_metric_fn: 0.4485 - lr: 9.8866e-05\n",
      "Epoch 97/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4398 - metric_fn: 0.4390 - val_loss: 0.4493 - val_metric_fn: 0.4485 - lr: 9.8866e-05\n",
      "Epoch 98/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4397 - metric_fn: 0.4390 - val_loss: 0.4493 - val_metric_fn: 0.4485 - lr: 9.8866e-05\n",
      "Epoch 99/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4395 - metric_fn: 0.4388 - val_loss: 0.4494 - val_metric_fn: 0.4486 - lr: 9.8866e-05\n",
      "Epoch 100/120\n",
      "244/248 [============================>.] - ETA: 0s - loss: 0.4394 - metric_fn: 0.4386Restoring model weights from the end of the best epoch: 70.\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4396 - metric_fn: 0.4388 - val_loss: 0.4494 - val_metric_fn: 0.4485 - lr: 6.9206e-05\n",
      "Epoch 100: early stopping\n",
      "343/343 [==============================] - 1s 1ms/step\n",
      "-------------------2-------------------------\n",
      "Epoch 1/120\n",
      "248/248 [==============================] - 2s 4ms/step - loss: 0.5588 - metric_fn: 0.5476 - val_loss: 2.4293 - val_metric_fn: 2.4187 - lr: 0.0050\n",
      "Epoch 2/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4571 - metric_fn: 0.4470 - val_loss: 0.5598 - val_metric_fn: 0.5502 - lr: 0.0050\n",
      "Epoch 3/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4540 - metric_fn: 0.4450 - val_loss: 0.4573 - val_metric_fn: 0.4488 - lr: 0.0050\n",
      "Epoch 4/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4517 - metric_fn: 0.4438 - val_loss: 0.4588 - val_metric_fn: 0.4513 - lr: 0.0050\n",
      "Epoch 5/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4500 - metric_fn: 0.4430 - val_loss: 0.4553 - val_metric_fn: 0.4488 - lr: 0.0050\n",
      "Epoch 6/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4494 - metric_fn: 0.4431 - val_loss: 0.4552 - val_metric_fn: 0.4492 - lr: 0.0050\n",
      "Epoch 7/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4486 - metric_fn: 0.4429 - val_loss: 0.4530 - val_metric_fn: 0.4474 - lr: 0.0050\n",
      "Epoch 8/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4478 - metric_fn: 0.4426 - val_loss: 0.4527 - val_metric_fn: 0.4477 - lr: 0.0050\n",
      "Epoch 9/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4472 - metric_fn: 0.4423 - val_loss: 0.4544 - val_metric_fn: 0.4497 - lr: 0.0050\n",
      "Epoch 10/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4466 - metric_fn: 0.4422 - val_loss: 0.4526 - val_metric_fn: 0.4482 - lr: 0.0050\n",
      "Epoch 11/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4466 - metric_fn: 0.4424 - val_loss: 0.4526 - val_metric_fn: 0.4485 - lr: 0.0050\n",
      "Epoch 12/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4462 - metric_fn: 0.4422 - val_loss: 0.4522 - val_metric_fn: 0.4483 - lr: 0.0050\n",
      "Epoch 13/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4460 - metric_fn: 0.4423 - val_loss: 0.4511 - val_metric_fn: 0.4474 - lr: 0.0050\n",
      "Epoch 14/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4457 - metric_fn: 0.4423 - val_loss: 0.4516 - val_metric_fn: 0.4481 - lr: 0.0050\n",
      "Epoch 15/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4452 - metric_fn: 0.4419 - val_loss: 0.4517 - val_metric_fn: 0.4484 - lr: 0.0050\n",
      "Epoch 16/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4454 - metric_fn: 0.4422 - val_loss: 0.4501 - val_metric_fn: 0.4470 - lr: 0.0050\n",
      "Epoch 17/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4451 - metric_fn: 0.4421 - val_loss: 0.4510 - val_metric_fn: 0.4481 - lr: 0.0050\n",
      "Epoch 18/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4444 - metric_fn: 0.4415 - val_loss: 0.4500 - val_metric_fn: 0.4473 - lr: 0.0050\n",
      "Epoch 19/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4444 - metric_fn: 0.4416 - val_loss: 0.4497 - val_metric_fn: 0.4471 - lr: 0.0050\n",
      "Epoch 20/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4446 - metric_fn: 0.4420 - val_loss: 0.4518 - val_metric_fn: 0.4493 - lr: 0.0050\n",
      "Epoch 21/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4446 - metric_fn: 0.4422 - val_loss: 0.4505 - val_metric_fn: 0.4481 - lr: 0.0050\n",
      "Epoch 22/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4441 - metric_fn: 0.4418 - val_loss: 0.4497 - val_metric_fn: 0.4474 - lr: 0.0050\n",
      "Epoch 23/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4444 - metric_fn: 0.4422 - val_loss: 0.4503 - val_metric_fn: 0.4480 - lr: 0.0050\n",
      "Epoch 24/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4436 - metric_fn: 0.4416 - val_loss: 0.4487 - val_metric_fn: 0.4467 - lr: 0.0035\n",
      "Epoch 25/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4434 - metric_fn: 0.4417 - val_loss: 0.4493 - val_metric_fn: 0.4474 - lr: 0.0035\n",
      "Epoch 26/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4435 - metric_fn: 0.4416 - val_loss: 0.4487 - val_metric_fn: 0.4468 - lr: 0.0035\n",
      "Epoch 27/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4432 - metric_fn: 0.4414 - val_loss: 0.4486 - val_metric_fn: 0.4468 - lr: 0.0035\n",
      "Epoch 28/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4431 - metric_fn: 0.4415 - val_loss: 0.4492 - val_metric_fn: 0.4474 - lr: 0.0035\n",
      "Epoch 29/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4432 - metric_fn: 0.4415 - val_loss: 0.4492 - val_metric_fn: 0.4475 - lr: 0.0035\n",
      "Epoch 30/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4430 - metric_fn: 0.4415 - val_loss: 0.4484 - val_metric_fn: 0.4469 - lr: 0.0035\n",
      "Epoch 31/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4427 - metric_fn: 0.4413 - val_loss: 0.4488 - val_metric_fn: 0.4473 - lr: 0.0035\n",
      "Epoch 32/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4427 - metric_fn: 0.4413 - val_loss: 0.4498 - val_metric_fn: 0.4483 - lr: 0.0035\n",
      "Epoch 33/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4425 - metric_fn: 0.4410 - val_loss: 0.4479 - val_metric_fn: 0.4464 - lr: 0.0035\n",
      "Epoch 34/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4428 - metric_fn: 0.4413 - val_loss: 0.4480 - val_metric_fn: 0.4466 - lr: 0.0035\n",
      "Epoch 35/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4425 - metric_fn: 0.4410 - val_loss: 0.4478 - val_metric_fn: 0.4463 - lr: 0.0035\n",
      "Epoch 36/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4431 - metric_fn: 0.4417 - val_loss: 0.4487 - val_metric_fn: 0.4473 - lr: 0.0035\n",
      "Epoch 37/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4425 - metric_fn: 0.4412 - val_loss: 0.4484 - val_metric_fn: 0.4471 - lr: 0.0035\n",
      "Epoch 38/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4423 - metric_fn: 0.4411 - val_loss: 0.4480 - val_metric_fn: 0.4467 - lr: 0.0025\n",
      "Epoch 39/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4421 - metric_fn: 0.4409 - val_loss: 0.4478 - val_metric_fn: 0.4465 - lr: 0.0025\n",
      "Epoch 40/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4423 - metric_fn: 0.4410 - val_loss: 0.4480 - val_metric_fn: 0.4467 - lr: 0.0025\n",
      "Epoch 41/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4420 - metric_fn: 0.4410 - val_loss: 0.4477 - val_metric_fn: 0.4464 - lr: 0.0025\n",
      "Epoch 42/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4424 - metric_fn: 0.4412 - val_loss: 0.4483 - val_metric_fn: 0.4471 - lr: 0.0025\n",
      "Epoch 43/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4423 - metric_fn: 0.4412 - val_loss: 0.4482 - val_metric_fn: 0.4470 - lr: 0.0025\n",
      "Epoch 44/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4421 - metric_fn: 0.4410 - val_loss: 0.4475 - val_metric_fn: 0.4463 - lr: 0.0025\n",
      "Epoch 45/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4423 - metric_fn: 0.4411 - val_loss: 0.4476 - val_metric_fn: 0.4465 - lr: 0.0025\n",
      "Epoch 46/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4421 - metric_fn: 0.4409 - val_loss: 0.4476 - val_metric_fn: 0.4465 - lr: 0.0025\n",
      "Epoch 47/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4417 - metric_fn: 0.4408 - val_loss: 0.4474 - val_metric_fn: 0.4463 - lr: 0.0025\n",
      "Epoch 48/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4421 - metric_fn: 0.4411 - val_loss: 0.4471 - val_metric_fn: 0.4460 - lr: 0.0025\n",
      "Epoch 49/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4421 - metric_fn: 0.4410 - val_loss: 0.4470 - val_metric_fn: 0.4459 - lr: 0.0025\n",
      "Epoch 50/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4418 - metric_fn: 0.4409 - val_loss: 0.4473 - val_metric_fn: 0.4462 - lr: 0.0025\n",
      "Epoch 51/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4424 - metric_fn: 0.4413 - val_loss: 0.4475 - val_metric_fn: 0.4464 - lr: 0.0025\n",
      "Epoch 52/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4419 - metric_fn: 0.4409 - val_loss: 0.4473 - val_metric_fn: 0.4462 - lr: 0.0025\n",
      "Epoch 53/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4415 - metric_fn: 0.4404 - val_loss: 0.4475 - val_metric_fn: 0.4464 - lr: 0.0017\n",
      "Epoch 54/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4413 - metric_fn: 0.4404 - val_loss: 0.4471 - val_metric_fn: 0.4461 - lr: 0.0017\n",
      "Epoch 55/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4417 - metric_fn: 0.4407 - val_loss: 0.4471 - val_metric_fn: 0.4461 - lr: 0.0017\n",
      "Epoch 56/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4416 - metric_fn: 0.4407 - val_loss: 0.4478 - val_metric_fn: 0.4468 - lr: 0.0017\n",
      "Epoch 57/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4414 - metric_fn: 0.4404 - val_loss: 0.4470 - val_metric_fn: 0.4460 - lr: 0.0012\n",
      "Epoch 58/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4414 - metric_fn: 0.4404 - val_loss: 0.4471 - val_metric_fn: 0.4461 - lr: 0.0012\n",
      "Epoch 59/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4412 - metric_fn: 0.4404 - val_loss: 0.4474 - val_metric_fn: 0.4465 - lr: 0.0012\n",
      "Epoch 60/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4413 - metric_fn: 0.4403 - val_loss: 0.4471 - val_metric_fn: 0.4461 - lr: 0.0012\n",
      "Epoch 61/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4409 - metric_fn: 0.4399 - val_loss: 0.4469 - val_metric_fn: 0.4460 - lr: 8.4035e-04\n",
      "Epoch 62/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4410 - metric_fn: 0.4402 - val_loss: 0.4469 - val_metric_fn: 0.4460 - lr: 8.4035e-04\n",
      "Epoch 63/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4413 - metric_fn: 0.4406 - val_loss: 0.4475 - val_metric_fn: 0.4465 - lr: 8.4035e-04\n",
      "Epoch 64/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4411 - metric_fn: 0.4402 - val_loss: 0.4473 - val_metric_fn: 0.4463 - lr: 8.4035e-04\n",
      "Epoch 65/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4411 - metric_fn: 0.4402 - val_loss: 0.4469 - val_metric_fn: 0.4460 - lr: 8.4035e-04\n",
      "Epoch 66/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4412 - metric_fn: 0.4402 - val_loss: 0.4469 - val_metric_fn: 0.4461 - lr: 5.8825e-04\n",
      "Epoch 67/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4409 - metric_fn: 0.4400 - val_loss: 0.4471 - val_metric_fn: 0.4462 - lr: 5.8825e-04\n",
      "Epoch 68/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4410 - metric_fn: 0.4402 - val_loss: 0.4469 - val_metric_fn: 0.4461 - lr: 5.8825e-04\n",
      "Epoch 69/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4411 - metric_fn: 0.4403 - val_loss: 0.4471 - val_metric_fn: 0.4462 - lr: 5.8825e-04\n",
      "Epoch 70/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4407 - metric_fn: 0.4400 - val_loss: 0.4471 - val_metric_fn: 0.4462 - lr: 4.1177e-04\n",
      "Epoch 71/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4409 - metric_fn: 0.4401 - val_loss: 0.4470 - val_metric_fn: 0.4462 - lr: 4.1177e-04\n",
      "Epoch 72/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4410 - metric_fn: 0.4402 - val_loss: 0.4469 - val_metric_fn: 0.4461 - lr: 4.1177e-04\n",
      "Epoch 73/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4406 - metric_fn: 0.4399 - val_loss: 0.4469 - val_metric_fn: 0.4461 - lr: 4.1177e-04\n",
      "Epoch 74/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4409 - metric_fn: 0.4402 - val_loss: 0.4471 - val_metric_fn: 0.4463 - lr: 2.8824e-04\n",
      "Epoch 75/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4404 - metric_fn: 0.4396 - val_loss: 0.4471 - val_metric_fn: 0.4462 - lr: 2.8824e-04\n",
      "Epoch 76/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4407 - metric_fn: 0.4400 - val_loss: 0.4470 - val_metric_fn: 0.4462 - lr: 2.8824e-04\n",
      "Epoch 77/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4407 - metric_fn: 0.4398 - val_loss: 0.4470 - val_metric_fn: 0.4462 - lr: 2.8824e-04\n",
      "Epoch 78/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4403 - metric_fn: 0.4395 - val_loss: 0.4470 - val_metric_fn: 0.4462 - lr: 2.0177e-04\n",
      "Epoch 79/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4406 - metric_fn: 0.4398 - val_loss: 0.4470 - val_metric_fn: 0.4461 - lr: 2.0177e-04\n",
      "Epoch 80/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4406 - metric_fn: 0.4399 - val_loss: 0.4470 - val_metric_fn: 0.4462 - lr: 2.0177e-04\n",
      "Epoch 81/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4404 - metric_fn: 0.4396 - val_loss: 0.4470 - val_metric_fn: 0.4462 - lr: 2.0177e-04\n",
      "Epoch 82/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4402 - metric_fn: 0.4395 - val_loss: 0.4471 - val_metric_fn: 0.4463 - lr: 1.4124e-04\n",
      "Epoch 83/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4402 - metric_fn: 0.4395 - val_loss: 0.4470 - val_metric_fn: 0.4462 - lr: 1.4124e-04\n",
      "Epoch 84/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4406 - metric_fn: 0.4399 - val_loss: 0.4470 - val_metric_fn: 0.4462 - lr: 1.4124e-04\n",
      "Epoch 85/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4402 - metric_fn: 0.4395 - val_loss: 0.4470 - val_metric_fn: 0.4462 - lr: 1.4124e-04\n",
      "Epoch 86/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4402 - metric_fn: 0.4394 - val_loss: 0.4470 - val_metric_fn: 0.4462 - lr: 9.8866e-05\n",
      "Epoch 87/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4404 - metric_fn: 0.4397 - val_loss: 0.4471 - val_metric_fn: 0.4463 - lr: 9.8866e-05\n",
      "Epoch 88/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4404 - metric_fn: 0.4399 - val_loss: 0.4471 - val_metric_fn: 0.4463 - lr: 9.8866e-05\n",
      "Epoch 89/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4402 - metric_fn: 0.4394 - val_loss: 0.4471 - val_metric_fn: 0.4463 - lr: 9.8866e-05\n",
      "Epoch 90/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4402 - metric_fn: 0.4394 - val_loss: 0.4471 - val_metric_fn: 0.4463 - lr: 6.9206e-05\n",
      "Epoch 91/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4403 - metric_fn: 0.4397 - val_loss: 0.4471 - val_metric_fn: 0.4463 - lr: 6.9206e-05\n",
      "Epoch 92/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4403 - metric_fn: 0.4396 - val_loss: 0.4471 - val_metric_fn: 0.4463 - lr: 6.9206e-05\n",
      "Epoch 93/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4402 - metric_fn: 0.4392 - val_loss: 0.4471 - val_metric_fn: 0.4463 - lr: 6.9206e-05\n",
      "Epoch 94/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4403 - metric_fn: 0.4398 - val_loss: 0.4471 - val_metric_fn: 0.4463 - lr: 4.8445e-05\n",
      "Epoch 95/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4403 - metric_fn: 0.4396 - val_loss: 0.4471 - val_metric_fn: 0.4463 - lr: 4.8445e-05\n",
      "Epoch 96/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4404 - metric_fn: 0.4397 - val_loss: 0.4472 - val_metric_fn: 0.4464 - lr: 4.8445e-05\n",
      "Epoch 97/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4404 - metric_fn: 0.4397 - val_loss: 0.4471 - val_metric_fn: 0.4463 - lr: 4.8445e-05\n",
      "Epoch 98/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4404 - metric_fn: 0.4397 - val_loss: 0.4472 - val_metric_fn: 0.4464 - lr: 3.3911e-05\n",
      "Epoch 99/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4401 - metric_fn: 0.4395 - val_loss: 0.4472 - val_metric_fn: 0.4464 - lr: 3.3911e-05\n",
      "Epoch 100/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4402 - metric_fn: 0.4395 - val_loss: 0.4472 - val_metric_fn: 0.4464 - lr: 3.3911e-05\n",
      "Epoch 101/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4402 - metric_fn: 0.4395 - val_loss: 0.4471 - val_metric_fn: 0.4464 - lr: 3.3911e-05\n",
      "Epoch 102/120\n",
      "240/248 [============================>.] - ETA: 0s - loss: 0.4407 - metric_fn: 0.4399Restoring model weights from the end of the best epoch: 72.\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4402 - metric_fn: 0.4394 - val_loss: 0.4471 - val_metric_fn: 0.4463 - lr: 2.3738e-05\n",
      "Epoch 102: early stopping\n",
      "343/343 [==============================] - 1s 1ms/step\n",
      "-------------------3-------------------------\n",
      "Epoch 1/120\n",
      "248/248 [==============================] - 3s 5ms/step - loss: 0.5147 - metric_fn: 0.5030 - val_loss: 1.9031 - val_metric_fn: 1.8921 - lr: 0.0050\n",
      "Epoch 2/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4606 - metric_fn: 0.4500 - val_loss: 0.4678 - val_metric_fn: 0.4578 - lr: 0.0050\n",
      "Epoch 3/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4573 - metric_fn: 0.4478 - val_loss: 0.4490 - val_metric_fn: 0.4399 - lr: 0.0050\n",
      "Epoch 4/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4558 - metric_fn: 0.4472 - val_loss: 0.4466 - val_metric_fn: 0.4382 - lr: 0.0050\n",
      "Epoch 5/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4549 - metric_fn: 0.4470 - val_loss: 0.4431 - val_metric_fn: 0.4354 - lr: 0.0050\n",
      "Epoch 6/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4537 - metric_fn: 0.4464 - val_loss: 0.4417 - val_metric_fn: 0.4346 - lr: 0.0050\n",
      "Epoch 7/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4522 - metric_fn: 0.4455 - val_loss: 0.4418 - val_metric_fn: 0.4353 - lr: 0.0050\n",
      "Epoch 8/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4517 - metric_fn: 0.4455 - val_loss: 0.4418 - val_metric_fn: 0.4358 - lr: 0.0050\n",
      "Epoch 9/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4513 - metric_fn: 0.4455 - val_loss: 0.4419 - val_metric_fn: 0.4363 - lr: 0.0050\n",
      "Epoch 10/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4509 - metric_fn: 0.4457 - val_loss: 0.4430 - val_metric_fn: 0.4376 - lr: 0.0050\n",
      "Epoch 11/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4496 - metric_fn: 0.4446 - val_loss: 0.4397 - val_metric_fn: 0.4347 - lr: 0.0035\n",
      "Epoch 12/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4491 - metric_fn: 0.4443 - val_loss: 0.4418 - val_metric_fn: 0.4370 - lr: 0.0035\n",
      "Epoch 13/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4487 - metric_fn: 0.4442 - val_loss: 0.4408 - val_metric_fn: 0.4362 - lr: 0.0035\n",
      "Epoch 14/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4489 - metric_fn: 0.4444 - val_loss: 0.4409 - val_metric_fn: 0.4365 - lr: 0.0035\n",
      "Epoch 15/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4487 - metric_fn: 0.4444 - val_loss: 0.4398 - val_metric_fn: 0.4355 - lr: 0.0035\n",
      "Epoch 16/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4476 - metric_fn: 0.4434 - val_loss: 0.4393 - val_metric_fn: 0.4352 - lr: 0.0025\n",
      "Epoch 17/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4472 - metric_fn: 0.4432 - val_loss: 0.4421 - val_metric_fn: 0.4381 - lr: 0.0025\n",
      "Epoch 18/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4471 - metric_fn: 0.4432 - val_loss: 0.4387 - val_metric_fn: 0.4349 - lr: 0.0025\n",
      "Epoch 19/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4471 - metric_fn: 0.4434 - val_loss: 0.4380 - val_metric_fn: 0.4342 - lr: 0.0025\n",
      "Epoch 20/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4471 - metric_fn: 0.4436 - val_loss: 0.4395 - val_metric_fn: 0.4358 - lr: 0.0025\n",
      "Epoch 21/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4469 - metric_fn: 0.4435 - val_loss: 0.4394 - val_metric_fn: 0.4358 - lr: 0.0025\n",
      "Epoch 22/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4467 - metric_fn: 0.4432 - val_loss: 0.4381 - val_metric_fn: 0.4345 - lr: 0.0025\n",
      "Epoch 23/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4467 - metric_fn: 0.4432 - val_loss: 0.4395 - val_metric_fn: 0.4360 - lr: 0.0025\n",
      "Epoch 24/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4462 - metric_fn: 0.4430 - val_loss: 0.4371 - val_metric_fn: 0.4338 - lr: 0.0017\n",
      "Epoch 25/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4456 - metric_fn: 0.4423 - val_loss: 0.4382 - val_metric_fn: 0.4349 - lr: 0.0017\n",
      "Epoch 26/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4457 - metric_fn: 0.4426 - val_loss: 0.4377 - val_metric_fn: 0.4345 - lr: 0.0017\n",
      "Epoch 27/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4457 - metric_fn: 0.4426 - val_loss: 0.4377 - val_metric_fn: 0.4345 - lr: 0.0017\n",
      "Epoch 28/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4459 - metric_fn: 0.4428 - val_loss: 0.4381 - val_metric_fn: 0.4349 - lr: 0.0017\n",
      "Epoch 29/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4452 - metric_fn: 0.4422 - val_loss: 0.4376 - val_metric_fn: 0.4345 - lr: 0.0012\n",
      "Epoch 30/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4448 - metric_fn: 0.4420 - val_loss: 0.4379 - val_metric_fn: 0.4349 - lr: 0.0012\n",
      "Epoch 31/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4446 - metric_fn: 0.4418 - val_loss: 0.4383 - val_metric_fn: 0.4354 - lr: 0.0012\n",
      "Epoch 32/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4447 - metric_fn: 0.4417 - val_loss: 0.4372 - val_metric_fn: 0.4343 - lr: 0.0012\n",
      "Epoch 33/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4443 - metric_fn: 0.4415 - val_loss: 0.4375 - val_metric_fn: 0.4346 - lr: 8.4035e-04\n",
      "Epoch 34/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4441 - metric_fn: 0.4414 - val_loss: 0.4375 - val_metric_fn: 0.4347 - lr: 8.4035e-04\n",
      "Epoch 35/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4441 - metric_fn: 0.4414 - val_loss: 0.4375 - val_metric_fn: 0.4347 - lr: 8.4035e-04\n",
      "Epoch 36/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4436 - metric_fn: 0.4409 - val_loss: 0.4381 - val_metric_fn: 0.4354 - lr: 8.4035e-04\n",
      "Epoch 37/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4434 - metric_fn: 0.4408 - val_loss: 0.4375 - val_metric_fn: 0.4348 - lr: 5.8825e-04\n",
      "Epoch 38/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4433 - metric_fn: 0.4408 - val_loss: 0.4370 - val_metric_fn: 0.4343 - lr: 5.8825e-04\n",
      "Epoch 39/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4435 - metric_fn: 0.4409 - val_loss: 0.4381 - val_metric_fn: 0.4355 - lr: 5.8825e-04\n",
      "Epoch 40/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4431 - metric_fn: 0.4407 - val_loss: 0.4377 - val_metric_fn: 0.4351 - lr: 5.8825e-04\n",
      "Epoch 41/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4431 - metric_fn: 0.4406 - val_loss: 0.4375 - val_metric_fn: 0.4349 - lr: 5.8825e-04\n",
      "Epoch 42/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4432 - metric_fn: 0.4407 - val_loss: 0.4370 - val_metric_fn: 0.4345 - lr: 5.8825e-04\n",
      "Epoch 43/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4426 - metric_fn: 0.4402 - val_loss: 0.4370 - val_metric_fn: 0.4344 - lr: 4.1177e-04\n",
      "Epoch 44/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4427 - metric_fn: 0.4403 - val_loss: 0.4376 - val_metric_fn: 0.4351 - lr: 4.1177e-04\n",
      "Epoch 45/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4428 - metric_fn: 0.4404 - val_loss: 0.4371 - val_metric_fn: 0.4346 - lr: 4.1177e-04\n",
      "Epoch 46/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4425 - metric_fn: 0.4402 - val_loss: 0.4372 - val_metric_fn: 0.4347 - lr: 4.1177e-04\n",
      "Epoch 47/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4422 - metric_fn: 0.4397 - val_loss: 0.4375 - val_metric_fn: 0.4350 - lr: 2.8824e-04\n",
      "Epoch 48/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4423 - metric_fn: 0.4400 - val_loss: 0.4371 - val_metric_fn: 0.4347 - lr: 2.8824e-04\n",
      "Epoch 49/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4421 - metric_fn: 0.4398 - val_loss: 0.4373 - val_metric_fn: 0.4349 - lr: 2.8824e-04\n",
      "Epoch 50/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4423 - metric_fn: 0.4399 - val_loss: 0.4369 - val_metric_fn: 0.4345 - lr: 2.8824e-04\n",
      "Epoch 51/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4419 - metric_fn: 0.4395 - val_loss: 0.4371 - val_metric_fn: 0.4347 - lr: 2.0177e-04\n",
      "Epoch 52/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4417 - metric_fn: 0.4393 - val_loss: 0.4372 - val_metric_fn: 0.4348 - lr: 2.0177e-04\n",
      "Epoch 53/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4419 - metric_fn: 0.4395 - val_loss: 0.4374 - val_metric_fn: 0.4351 - lr: 2.0177e-04\n",
      "Epoch 54/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4420 - metric_fn: 0.4399 - val_loss: 0.4373 - val_metric_fn: 0.4350 - lr: 2.0177e-04\n",
      "Epoch 55/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4416 - metric_fn: 0.4394 - val_loss: 0.4373 - val_metric_fn: 0.4350 - lr: 1.4124e-04\n",
      "Epoch 56/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4415 - metric_fn: 0.4393 - val_loss: 0.4374 - val_metric_fn: 0.4350 - lr: 1.4124e-04\n",
      "Epoch 57/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4419 - metric_fn: 0.4395 - val_loss: 0.4374 - val_metric_fn: 0.4351 - lr: 1.4124e-04\n",
      "Epoch 58/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4416 - metric_fn: 0.4394 - val_loss: 0.4374 - val_metric_fn: 0.4350 - lr: 1.4124e-04\n",
      "Epoch 59/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4419 - metric_fn: 0.4396 - val_loss: 0.4374 - val_metric_fn: 0.4351 - lr: 9.8866e-05\n",
      "Epoch 60/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4413 - metric_fn: 0.4391 - val_loss: 0.4375 - val_metric_fn: 0.4352 - lr: 9.8866e-05\n",
      "Epoch 61/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4417 - metric_fn: 0.4394 - val_loss: 0.4373 - val_metric_fn: 0.4350 - lr: 9.8866e-05\n",
      "Epoch 62/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4414 - metric_fn: 0.4393 - val_loss: 0.4373 - val_metric_fn: 0.4350 - lr: 9.8866e-05\n",
      "Epoch 63/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4410 - metric_fn: 0.4386 - val_loss: 0.4374 - val_metric_fn: 0.4351 - lr: 6.9206e-05\n",
      "Epoch 64/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4413 - metric_fn: 0.4391 - val_loss: 0.4374 - val_metric_fn: 0.4351 - lr: 6.9206e-05\n",
      "Epoch 65/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4414 - metric_fn: 0.4392 - val_loss: 0.4374 - val_metric_fn: 0.4351 - lr: 6.9206e-05\n",
      "Epoch 66/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4414 - metric_fn: 0.4392 - val_loss: 0.4374 - val_metric_fn: 0.4351 - lr: 6.9206e-05\n",
      "Epoch 67/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4410 - metric_fn: 0.4388 - val_loss: 0.4375 - val_metric_fn: 0.4352 - lr: 4.8445e-05\n",
      "Epoch 68/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4411 - metric_fn: 0.4389 - val_loss: 0.4375 - val_metric_fn: 0.4352 - lr: 4.8445e-05\n",
      "Epoch 69/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4412 - metric_fn: 0.4391 - val_loss: 0.4374 - val_metric_fn: 0.4351 - lr: 4.8445e-05\n",
      "Epoch 70/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4411 - metric_fn: 0.4388 - val_loss: 0.4374 - val_metric_fn: 0.4351 - lr: 4.8445e-05\n",
      "Epoch 71/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4408 - metric_fn: 0.4387 - val_loss: 0.4374 - val_metric_fn: 0.4351 - lr: 3.3911e-05\n",
      "Epoch 72/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4409 - metric_fn: 0.4388 - val_loss: 0.4374 - val_metric_fn: 0.4351 - lr: 3.3911e-05\n",
      "Epoch 73/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4409 - metric_fn: 0.4387 - val_loss: 0.4374 - val_metric_fn: 0.4351 - lr: 3.3911e-05\n",
      "Epoch 74/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4411 - metric_fn: 0.4388 - val_loss: 0.4374 - val_metric_fn: 0.4352 - lr: 3.3911e-05\n",
      "Epoch 75/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4408 - metric_fn: 0.4387 - val_loss: 0.4374 - val_metric_fn: 0.4352 - lr: 2.3738e-05\n",
      "Epoch 76/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4411 - metric_fn: 0.4388 - val_loss: 0.4375 - val_metric_fn: 0.4352 - lr: 2.3738e-05\n",
      "Epoch 77/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4412 - metric_fn: 0.4391 - val_loss: 0.4374 - val_metric_fn: 0.4352 - lr: 2.3738e-05\n",
      "Epoch 78/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4412 - metric_fn: 0.4389 - val_loss: 0.4375 - val_metric_fn: 0.4352 - lr: 2.3738e-05\n",
      "Epoch 79/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4410 - metric_fn: 0.4389 - val_loss: 0.4375 - val_metric_fn: 0.4352 - lr: 1.6616e-05\n",
      "Epoch 80/120\n",
      "242/248 [============================>.] - ETA: 0s - loss: 0.4412 - metric_fn: 0.4390Restoring model weights from the end of the best epoch: 50.\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4412 - metric_fn: 0.4391 - val_loss: 0.4374 - val_metric_fn: 0.4352 - lr: 1.6616e-05\n",
      "Epoch 80: early stopping\n",
      "343/343 [==============================] - 1s 1ms/step\n",
      "-------------------4-------------------------\n",
      "Epoch 1/120\n",
      "248/248 [==============================] - 2s 4ms/step - loss: 0.5239 - metric_fn: 0.5123 - val_loss: 2.1587 - val_metric_fn: 2.1475 - lr: 0.0050\n",
      "Epoch 2/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4583 - metric_fn: 0.4477 - val_loss: 0.5152 - val_metric_fn: 0.5052 - lr: 0.0050\n",
      "Epoch 3/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4553 - metric_fn: 0.4459 - val_loss: 0.4547 - val_metric_fn: 0.4456 - lr: 0.0050\n",
      "Epoch 4/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4540 - metric_fn: 0.4456 - val_loss: 0.4529 - val_metric_fn: 0.4447 - lr: 0.0050\n",
      "Epoch 5/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4529 - metric_fn: 0.4453 - val_loss: 0.4515 - val_metric_fn: 0.4441 - lr: 0.0050\n",
      "Epoch 6/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4520 - metric_fn: 0.4450 - val_loss: 0.4508 - val_metric_fn: 0.4439 - lr: 0.0050\n",
      "Epoch 7/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4512 - metric_fn: 0.4447 - val_loss: 0.4519 - val_metric_fn: 0.4456 - lr: 0.0050\n",
      "Epoch 8/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4504 - metric_fn: 0.4446 - val_loss: 0.4499 - val_metric_fn: 0.4440 - lr: 0.0050\n",
      "Epoch 9/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4499 - metric_fn: 0.4442 - val_loss: 0.4480 - val_metric_fn: 0.4425 - lr: 0.0050\n",
      "Epoch 10/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4493 - metric_fn: 0.4441 - val_loss: 0.4477 - val_metric_fn: 0.4426 - lr: 0.0050\n",
      "Epoch 11/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4489 - metric_fn: 0.4441 - val_loss: 0.4465 - val_metric_fn: 0.4415 - lr: 0.0050\n",
      "Epoch 12/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4486 - metric_fn: 0.4440 - val_loss: 0.4462 - val_metric_fn: 0.4415 - lr: 0.0050\n",
      "Epoch 13/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4483 - metric_fn: 0.4438 - val_loss: 0.4460 - val_metric_fn: 0.4417 - lr: 0.0050\n",
      "Epoch 14/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4476 - metric_fn: 0.4436 - val_loss: 0.4471 - val_metric_fn: 0.4430 - lr: 0.0050\n",
      "Epoch 15/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4476 - metric_fn: 0.4437 - val_loss: 0.4462 - val_metric_fn: 0.4423 - lr: 0.0050\n",
      "Epoch 16/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4469 - metric_fn: 0.4432 - val_loss: 0.4443 - val_metric_fn: 0.4406 - lr: 0.0050\n",
      "Epoch 17/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4472 - metric_fn: 0.4437 - val_loss: 0.4449 - val_metric_fn: 0.4414 - lr: 0.0050\n",
      "Epoch 18/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4469 - metric_fn: 0.4436 - val_loss: 0.4436 - val_metric_fn: 0.4403 - lr: 0.0050\n",
      "Epoch 19/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4464 - metric_fn: 0.4433 - val_loss: 0.4452 - val_metric_fn: 0.4420 - lr: 0.0050\n",
      "Epoch 20/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4463 - metric_fn: 0.4434 - val_loss: 0.4446 - val_metric_fn: 0.4417 - lr: 0.0050\n",
      "Epoch 21/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4464 - metric_fn: 0.4435 - val_loss: 0.4442 - val_metric_fn: 0.4414 - lr: 0.0050\n",
      "Epoch 22/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4461 - metric_fn: 0.4436 - val_loss: 0.4434 - val_metric_fn: 0.4407 - lr: 0.0050\n",
      "Epoch 23/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4455 - metric_fn: 0.4430 - val_loss: 0.4430 - val_metric_fn: 0.4405 - lr: 0.0050\n",
      "Epoch 24/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4462 - metric_fn: 0.4437 - val_loss: 0.4432 - val_metric_fn: 0.4407 - lr: 0.0050\n",
      "Epoch 25/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4454 - metric_fn: 0.4431 - val_loss: 0.4432 - val_metric_fn: 0.4408 - lr: 0.0050\n",
      "Epoch 26/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4455 - metric_fn: 0.4433 - val_loss: 0.4433 - val_metric_fn: 0.4411 - lr: 0.0050\n",
      "Epoch 27/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4453 - metric_fn: 0.4433 - val_loss: 0.4422 - val_metric_fn: 0.4401 - lr: 0.0050\n",
      "Epoch 28/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4449 - metric_fn: 0.4431 - val_loss: 0.4427 - val_metric_fn: 0.4408 - lr: 0.0050\n",
      "Epoch 29/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4452 - metric_fn: 0.4432 - val_loss: 0.4423 - val_metric_fn: 0.4403 - lr: 0.0050\n",
      "Epoch 30/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4453 - metric_fn: 0.4433 - val_loss: 0.4444 - val_metric_fn: 0.4425 - lr: 0.0050\n",
      "Epoch 31/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4449 - metric_fn: 0.4430 - val_loss: 0.4427 - val_metric_fn: 0.4407 - lr: 0.0050\n",
      "Epoch 32/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4441 - metric_fn: 0.4425 - val_loss: 0.4419 - val_metric_fn: 0.4402 - lr: 0.0035\n",
      "Epoch 33/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4444 - metric_fn: 0.4427 - val_loss: 0.4420 - val_metric_fn: 0.4403 - lr: 0.0035\n",
      "Epoch 34/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4447 - metric_fn: 0.4430 - val_loss: 0.4425 - val_metric_fn: 0.4409 - lr: 0.0035\n",
      "Epoch 35/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4443 - metric_fn: 0.4429 - val_loss: 0.4417 - val_metric_fn: 0.4400 - lr: 0.0035\n",
      "Epoch 36/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4448 - metric_fn: 0.4433 - val_loss: 0.4422 - val_metric_fn: 0.4406 - lr: 0.0035\n",
      "Epoch 37/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4443 - metric_fn: 0.4428 - val_loss: 0.4417 - val_metric_fn: 0.4401 - lr: 0.0035\n",
      "Epoch 38/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4444 - metric_fn: 0.4429 - val_loss: 0.4421 - val_metric_fn: 0.4406 - lr: 0.0035\n",
      "Epoch 39/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4444 - metric_fn: 0.4429 - val_loss: 0.4417 - val_metric_fn: 0.4401 - lr: 0.0035\n",
      "Epoch 40/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4438 - metric_fn: 0.4425 - val_loss: 0.4416 - val_metric_fn: 0.4401 - lr: 0.0025\n",
      "Epoch 41/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4438 - metric_fn: 0.4424 - val_loss: 0.4417 - val_metric_fn: 0.4403 - lr: 0.0025\n",
      "Epoch 42/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4439 - metric_fn: 0.4425 - val_loss: 0.4417 - val_metric_fn: 0.4403 - lr: 0.0025\n",
      "Epoch 43/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4438 - metric_fn: 0.4426 - val_loss: 0.4423 - val_metric_fn: 0.4409 - lr: 0.0025\n",
      "Epoch 44/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4438 - metric_fn: 0.4424 - val_loss: 0.4420 - val_metric_fn: 0.4406 - lr: 0.0025\n",
      "Epoch 45/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4436 - metric_fn: 0.4424 - val_loss: 0.4417 - val_metric_fn: 0.4404 - lr: 0.0017\n",
      "Epoch 46/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4431 - metric_fn: 0.4420 - val_loss: 0.4418 - val_metric_fn: 0.4405 - lr: 0.0017\n",
      "Epoch 47/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4432 - metric_fn: 0.4421 - val_loss: 0.4413 - val_metric_fn: 0.4401 - lr: 0.0017\n",
      "Epoch 48/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4429 - metric_fn: 0.4418 - val_loss: 0.4414 - val_metric_fn: 0.4402 - lr: 0.0017\n",
      "Epoch 49/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4429 - metric_fn: 0.4419 - val_loss: 0.4421 - val_metric_fn: 0.4410 - lr: 0.0017\n",
      "Epoch 50/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4432 - metric_fn: 0.4422 - val_loss: 0.4428 - val_metric_fn: 0.4415 - lr: 0.0017\n",
      "Epoch 51/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4431 - metric_fn: 0.4421 - val_loss: 0.4417 - val_metric_fn: 0.4405 - lr: 0.0017\n",
      "Epoch 52/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4430 - metric_fn: 0.4420 - val_loss: 0.4413 - val_metric_fn: 0.4401 - lr: 0.0012\n",
      "Epoch 53/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4428 - metric_fn: 0.4419 - val_loss: 0.4416 - val_metric_fn: 0.4405 - lr: 0.0012\n",
      "Epoch 54/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4429 - metric_fn: 0.4418 - val_loss: 0.4420 - val_metric_fn: 0.4409 - lr: 0.0012\n",
      "Epoch 55/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4428 - metric_fn: 0.4417 - val_loss: 0.4414 - val_metric_fn: 0.4403 - lr: 0.0012\n",
      "Epoch 56/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4426 - metric_fn: 0.4415 - val_loss: 0.4412 - val_metric_fn: 0.4401 - lr: 8.4035e-04\n",
      "Epoch 57/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4424 - metric_fn: 0.4414 - val_loss: 0.4411 - val_metric_fn: 0.4401 - lr: 8.4035e-04\n",
      "Epoch 58/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4425 - metric_fn: 0.4416 - val_loss: 0.4419 - val_metric_fn: 0.4408 - lr: 8.4035e-04\n",
      "Epoch 59/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4425 - metric_fn: 0.4416 - val_loss: 0.4410 - val_metric_fn: 0.4399 - lr: 8.4035e-04\n",
      "Epoch 60/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4423 - metric_fn: 0.4414 - val_loss: 0.4418 - val_metric_fn: 0.4408 - lr: 8.4035e-04\n",
      "Epoch 61/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4424 - metric_fn: 0.4414 - val_loss: 0.4413 - val_metric_fn: 0.4403 - lr: 8.4035e-04\n",
      "Epoch 62/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4423 - metric_fn: 0.4414 - val_loss: 0.4415 - val_metric_fn: 0.4405 - lr: 8.4035e-04\n",
      "Epoch 63/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4424 - metric_fn: 0.4413 - val_loss: 0.4414 - val_metric_fn: 0.4403 - lr: 8.4035e-04\n",
      "Epoch 64/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4423 - metric_fn: 0.4416 - val_loss: 0.4415 - val_metric_fn: 0.4405 - lr: 5.8825e-04\n",
      "Epoch 65/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4422 - metric_fn: 0.4413 - val_loss: 0.4415 - val_metric_fn: 0.4405 - lr: 5.8825e-04\n",
      "Epoch 66/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4423 - metric_fn: 0.4414 - val_loss: 0.4412 - val_metric_fn: 0.4402 - lr: 5.8825e-04\n",
      "Epoch 67/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4423 - metric_fn: 0.4413 - val_loss: 0.4412 - val_metric_fn: 0.4402 - lr: 5.8825e-04\n",
      "Epoch 68/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4417 - metric_fn: 0.4408 - val_loss: 0.4413 - val_metric_fn: 0.4404 - lr: 4.1177e-04\n",
      "Epoch 69/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4420 - metric_fn: 0.4412 - val_loss: 0.4412 - val_metric_fn: 0.4402 - lr: 4.1177e-04\n",
      "Epoch 70/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4419 - metric_fn: 0.4410 - val_loss: 0.4416 - val_metric_fn: 0.4406 - lr: 4.1177e-04\n",
      "Epoch 71/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4418 - metric_fn: 0.4409 - val_loss: 0.4413 - val_metric_fn: 0.4403 - lr: 4.1177e-04\n",
      "Epoch 72/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4420 - metric_fn: 0.4411 - val_loss: 0.4413 - val_metric_fn: 0.4403 - lr: 2.8824e-04\n",
      "Epoch 73/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4418 - metric_fn: 0.4408 - val_loss: 0.4413 - val_metric_fn: 0.4403 - lr: 2.8824e-04\n",
      "Epoch 74/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4417 - metric_fn: 0.4408 - val_loss: 0.4414 - val_metric_fn: 0.4404 - lr: 2.8824e-04\n",
      "Epoch 75/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4418 - metric_fn: 0.4409 - val_loss: 0.4415 - val_metric_fn: 0.4406 - lr: 2.8824e-04\n",
      "Epoch 76/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4417 - metric_fn: 0.4407 - val_loss: 0.4415 - val_metric_fn: 0.4405 - lr: 2.0177e-04\n",
      "Epoch 77/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4415 - metric_fn: 0.4407 - val_loss: 0.4414 - val_metric_fn: 0.4404 - lr: 2.0177e-04\n",
      "Epoch 78/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4419 - metric_fn: 0.4411 - val_loss: 0.4415 - val_metric_fn: 0.4405 - lr: 2.0177e-04\n",
      "Epoch 79/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4419 - metric_fn: 0.4411 - val_loss: 0.4415 - val_metric_fn: 0.4405 - lr: 2.0177e-04\n",
      "Epoch 80/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4417 - metric_fn: 0.4410 - val_loss: 0.4414 - val_metric_fn: 0.4405 - lr: 1.4124e-04\n",
      "Epoch 81/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4416 - metric_fn: 0.4406 - val_loss: 0.4415 - val_metric_fn: 0.4405 - lr: 1.4124e-04\n",
      "Epoch 82/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4417 - metric_fn: 0.4408 - val_loss: 0.4414 - val_metric_fn: 0.4405 - lr: 1.4124e-04\n",
      "Epoch 83/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4414 - metric_fn: 0.4405 - val_loss: 0.4415 - val_metric_fn: 0.4406 - lr: 1.4124e-04\n",
      "Epoch 84/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4415 - metric_fn: 0.4406 - val_loss: 0.4415 - val_metric_fn: 0.4406 - lr: 9.8866e-05\n",
      "Epoch 85/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4415 - metric_fn: 0.4407 - val_loss: 0.4415 - val_metric_fn: 0.4406 - lr: 9.8866e-05\n",
      "Epoch 86/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4413 - metric_fn: 0.4405 - val_loss: 0.4415 - val_metric_fn: 0.4406 - lr: 9.8866e-05\n",
      "Epoch 87/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4416 - metric_fn: 0.4407 - val_loss: 0.4416 - val_metric_fn: 0.4406 - lr: 9.8866e-05\n",
      "Epoch 88/120\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4415 - metric_fn: 0.4406 - val_loss: 0.4415 - val_metric_fn: 0.4406 - lr: 6.9206e-05\n",
      "Epoch 89/120\n",
      "244/248 [============================>.] - ETA: 0s - loss: 0.4421 - metric_fn: 0.4413Restoring model weights from the end of the best epoch: 59.\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.4418 - metric_fn: 0.4409 - val_loss: 0.4415 - val_metric_fn: 0.4406 - lr: 6.9206e-05\n",
      "Epoch 89: early stopping\n",
      "343/343 [==============================] - 1s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "skf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold_histories = []\n",
    "test_preds = np.zeros(len(test_x))\n",
    "\n",
    "for i, (train_index, valid_index) in enumerate(skf.split(train_x, train_y)):\n",
    "    print(f'-------------------{ i }-------------------------')\n",
    "    # print(train_index)\n",
    "    x_train_fold, x_valid_fold = train_x.iloc[train_index], train_x.iloc[valid_index]\n",
    "    y_train_fold, y_valid_fold = train_y.iloc[train_index], train_y.iloc[valid_index]\n",
    "    # print(x_train_fold.shape, x_valid_fold.shape, y_train_fold.shape,y_valid_fold.shape)\n",
    "    \n",
    "    optimized_model = create_model(best_params['l1_reg'], best_params['l2_reg'], best_params['learning_rate'])\n",
    "    history = optimized_model.fit(\n",
    "                            x_train_fold.astype('float32'), y_train_fold.astype('float32'),\n",
    "                            epochs=120, batch_size=best_params['batch_size'], verbose=1,\n",
    "                            validation_data=(x_valid_fold.astype('float32'), y_valid_fold.astype('float32')),\n",
    "                            callbacks=callbacks_list)\n",
    "    \n",
    "    fold_histories.append(history)\n",
    "    \n",
    "    test_preds +=  optimized_model.predict(test_x.astype('float32')).reshape(-1) / skf.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train loss, val loss 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], 'b-', label='loss')\n",
    "plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        3.661664\n",
       "1        3.343852\n",
       "2        4.753667\n",
       "3        4.080441\n",
       "4        4.511107\n",
       "           ...   \n",
       "10958    5.510316\n",
       "10959    4.316818\n",
       "10960    4.805575\n",
       "10961    4.549596\n",
       "10962    4.454959\n",
       "Name: ECLO, Length: 10963, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "sample_submission[\"ECLO\"] = test_preds\n",
    "\n",
    "sample_submission.to_csv(\"result/ONLYdeagu_kf5_BN3_2456_submission.csv\", index=False)\n",
    "sample_submission[\"ECLO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !TODO\n",
    "# scheduler\n",
    "\n",
    "# train set 개수 늘리기 or 대구 데이터로만 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "   if hasattr(layer, 'reset_parameters'):\n",
    "    print(f'Reset trainable parameters of layer = {layer}')\n",
    "    layer.reset_parameters()\n",
    "\n",
    "class SimpleConvNet(nn.Module):\n",
    "  '''\n",
    "    Simple Convolutional Neural Network\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Conv2d(1, 10, kernel_size=3),\n",
    "      nn.ReLU(),\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(26 * 26 * 10, 50),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(50, 20),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(20, 10)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)\n",
    "  \n",
    "  \n",
    "if __name__ == '__main__':\n",
    "  \n",
    "  # Configuration options\n",
    "  k_folds = 5\n",
    "  num_epochs = 1\n",
    "  loss_function = nn.CrossEntropyLoss()\n",
    "  \n",
    "  # For fold results\n",
    "  results = {}\n",
    "  \n",
    "  # Set fixed random number seed\n",
    "  torch.manual_seed(42)\n",
    "  \n",
    "  # Prepare MNIST dataset by concatenating Train/Test part; we split later.\n",
    "  dataset_train_part = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor(), train=True)\n",
    "  dataset_test_part = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor(), train=False)\n",
    "  dataset = ConcatDataset([dataset_train_part, dataset_test_part])\n",
    "  \n",
    "  # Define the K-fold Cross Validator\n",
    "  kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "    \n",
    "  # Start print\n",
    "  print('--------------------------------')\n",
    "\n",
    "  # K-fold Cross Validation model evaluation\n",
    "  for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    \n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "    \n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "                      dataset, \n",
    "                      batch_size=10, sampler=train_subsampler)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "                      dataset,\n",
    "                      batch_size=10, sampler=test_subsampler)\n",
    "    \n",
    "    # Init the neural network\n",
    "    network = SimpleConvNet()\n",
    "    network.apply(reset_weights)\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
    "    \n",
    "    # Run the training loop for defined number of epochs\n",
    "    for epoch in range(0, num_epochs):\n",
    "\n",
    "      # Print epoch\n",
    "      print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "      # Set current loss value\n",
    "      current_loss = 0.0\n",
    "\n",
    "      # Iterate over the DataLoader for training data\n",
    "      for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        # Get inputs\n",
    "        inputs, targets = data\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Perform forward pass\n",
    "        outputs = network(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "        \n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 500 == 499:\n",
    "            print('Loss after mini-batch %5d: %.3f' %\n",
    "                  (i + 1, current_loss / 500))\n",
    "            current_loss = 0.0\n",
    "            \n",
    "    # Process is complete.\n",
    "    print('Training process has finished. Saving trained model.')\n",
    "\n",
    "    # Print about testing\n",
    "    print('Starting testing')\n",
    "    \n",
    "    # Saving the model\n",
    "    save_path = f'./model-fold-{fold}.pth'\n",
    "    torch.save(network.state_dict(), save_path)\n",
    "\n",
    "    # Evaluationfor this fold\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "      # Iterate over the test data and generate predictions\n",
    "      for i, data in enumerate(testloader, 0):\n",
    "\n",
    "        # Get inputs\n",
    "        inputs, targets = data\n",
    "\n",
    "        # Generate outputs\n",
    "        outputs = network(inputs)\n",
    "\n",
    "        # Set total and correct\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "      # Print accuracy\n",
    "      print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n",
    "      print('--------------------------------')\n",
    "      results[fold] = 100.0 * (correct / total)\n",
    "    \n",
    "  # Print fold results\n",
    "  print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "  print('--------------------------------')\n",
    "  sum = 0.0\n",
    "  for key, value in results.items():\n",
    "    print(f'Fold {key}: {value} %')\n",
    "    sum += value\n",
    "  print(f'Average: {sum/len(results.items())} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daegu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
